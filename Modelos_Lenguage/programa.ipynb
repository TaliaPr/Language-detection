{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e3a9f2c",
   "metadata": {},
   "source": [
    "# Similitud Semàntica de Textos\n",
    "\n",
    "ELMO ==> EMBEDDING DE CARACTER ==> FILTRE CONVOLUCIONALS\n",
    "\n",
    "BERT ==> TOKENITZAR PER OBTENIR N-GRAMS (DICCIONARI MÉS CURT)\n",
    "\n",
    "HUGGINGFACE ==> MODELS PREENTRENATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd2a809",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer # EMBEDDING CONTEXTUAL FRASES AMB CÁLCUL DE DISTÀNCIA\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorboard.plugins import projector\n",
    "import numpy as np\n",
    "\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "from transformers import AutoModel, AutoTokenizer # tria automaticament el model i el tokenitzador"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35390ab1",
   "metadata": {},
   "source": [
    "## EJEMPLO MODELO CALSSIFICACION A PARTIR DE BERT + CAPA EXTRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9254b7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a67382d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divideix les dades en conjunts d'entrenament i validació\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(\n",
    "    sentences, labels, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32018098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base de dades vectorials per usar aquests models que generen vectors\n",
    "def compute_similarity(sentence1, sentence2):\n",
    "    # Carrega el model STS pre-entrenat\n",
    "    model_name = 'stsb-roberta-base'\n",
    "    model = SentenceTransformer(model_name)\n",
    "    # Codifica les frases i calcula la similitud del cosinus\n",
    "    embeddings1 = model.encode([sentence1])\n",
    "    embeddings2 = model.encode([sentence2])\n",
    "    similarity = cosine(embeddings1, embeddings2)\n",
    "    return similarity\n",
    "# Exemple d'ús\n",
    "sentence1 = \"I like cats\"\n",
    "sentence2 = \"I love dogs\"\n",
    "similarity = compute_similarity(sentence1, sentence2)\n",
    "print(f\"Similitud: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfd951f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepara les dades d'entrada en el format requerit per a NER\n",
    "def prepare_input(corpus):\n",
    "    sentences, labels = [], []\n",
    "    for tagged_sentence in corpus:\n",
    "        sentence, tag = zip(*tagged_sentence)\n",
    "        sentences.append(\" \".join(sentence))\n",
    "        labels.append(list(tag))\n",
    "    return sentences, labels\n",
    "sentences, labels = prepare_input(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7514ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "# Tokenitza i converteix les frases al format d'entrada\n",
    "train_encodings = tokenizer(train_sentences, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_sentences, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9188a6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels_to_ids(labels, label_map):\n",
    "    label_ids = []\n",
    "    for sent_labels in labels:\n",
    "        label_ids.append([label_map[label] for label in sent_labels])\n",
    "    return label_ids\n",
    "# Defineix la correspondència d'etiquetes. Ordena-ho per tenir O abans. I després B abans que I.\n",
    "labels_set = set(sum(labels, []))\n",
    "label_list = sorted(labels_set, key=lambda e: \"0\" if e == \"O\" else e[2]+e[0])\n",
    "label_map = {label: i for i, label in enumerate(label_list)}\n",
    "# Converteix les etiquetes a IDs d'etiquetes\n",
    "train_label_ids = convert_labels_to_ids(train_labels, label_map)\n",
    "val_label_ids = convert_labels_to_ids(val_labels, label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123dea19",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_ids = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    train_label_ids, padding=\"post\", truncating=\"post\", maxlen=tokenizer.model_max_length)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_label_ids\n",
    "))\n",
    "val_label_ids = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    val_label_ids, padding=\"post\", truncating=\"post\", maxlen=tokenizer.model_max_length)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    val_label_ids\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
