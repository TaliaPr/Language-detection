{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e1764a4",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER) Example\n",
    "\n",
    "This notebook demonstrates how to use the data processing classes and NER functionality implemented in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8edf90df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import conll2002\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from nltk.tag import CRFTagger\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Tuple\n",
    "import time\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c587182",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data\n",
    "\n",
    "First, we'll load data from the conll2002 corpus for Spanish NER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dcce6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 8323 sentences\n",
      "Test set: 1517 sentences\n",
      "\n",
      "Example sentence:\n",
      "[('Melbourne', 'NP', 'B-LOC'), ('(', 'Fpa', 'O'), ('Australia', 'NP', 'B-LOC'), (')', 'Fpt', 'O'), (',', 'Fc', 'O'), ('25', 'Z', 'O'), ('may', 'NC', 'O'), ('(', 'Fpa', 'O'), ('EFE', 'NC', 'B-ORG'), (')', 'Fpt', 'O'), ('.', 'Fp', 'O')]\n"
     ]
    }
   ],
   "source": [
    "# Load the Spanish NER data\n",
    "train = conll2002.iob_sents('esp.train')\n",
    "test = conll2002.iob_sents('esp.testb')\n",
    "\n",
    "print(f\"Training set: {len(train)} sentences\")\n",
    "print(f\"Test set: {len(test)} sentences\")\n",
    "\n",
    "# Show an example sentence\n",
    "print(\"\\nExample sentence:\")\n",
    "print(train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcda8b4d",
   "metadata": {},
   "source": [
    "## 2. Process Data Using Custom Classes\n",
    "\n",
    "Let's use our custom `data` class to process a sample sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2e400d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1504bd36",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering for NER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d18d70",
   "metadata": {},
   "source": [
    "Se va a implementar la clase encargada de construir el diccionario de features sobre los cuales se va a entrenar el modelo CRF.\n",
    "\n",
    "La clase de funciones de características para NER incluye:\n",
    "\n",
    "- **Características básicas de las palabras:** se van a incluir las características que vienen por defecto en nltk, ampliando un poco más de información que podemos saber de la palabra. Como por ejemplo, si está formada toda por letras mayusculas, si tiene signos de puntuación, etc.\n",
    "\n",
    "- **Características contextuales:** incluye aparte de la palabra actual, la palabra que le precede y la posterior.\n",
    "- **Características de etiquetas POS:** incluye el POS tag de la palabra actual y el de las palabras del contexto.\n",
    "- **Características de lemas:** incluye el lema de la palabra actual.\n",
    "- **Características específicas:** terminaciones de palabras o combinaciones de estas que podrian ayudar a detectar mejor las etiquetas que identifican a las localizaciónes o organizaciones.\n",
    "\n",
    "Esta clase es llamada **OptimizedFeatFunc**, también está optimizada deforma que cada vez que el CRFTagger vuelve a pasar por la misma frase la clase retorna el diccionario de características ya calculado anteriormente, esto evita volver a crear el diccionario.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4feea65b",
   "metadata": {},
   "source": [
    "Se analiza qué trigramas pueden preceder a la etiqueta que precede las entidades Organización (B-ORG'), estas se van a incluir en el apartado de las características específicas de OptimizedFeatFunc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823b292f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigramas de palabras más comunes que preceden a organizaciones:\n",
      "======================================================================\n",
      "23 may (: 680\n",
      "presidente de la: 72\n",
      "25 may (: 32\n",
      "El presidente del: 30\n",
      "el presidente del: 27\n",
      "general de la: 19\n",
      "director general de: 19\n",
      "el presidente de: 17\n",
      "de que el: 16\n",
      "fuentes de la: 16\n",
      "secretario general de: 16\n",
      "El presidente de: 14\n",
      "Unión Europea (: 14\n",
      "el portavoz del: 14\n",
      "por parte del: 12\n",
      "El portavoz del: 11\n",
      "miembros de la: 11\n",
      "portavoz de la: 11\n",
      "informó hoy el: 11\n",
      "representantes de la: 10\n",
      "Madrid y el: 10\n",
      "el jefe del: 10\n",
      "mientras que el: 10\n",
      "\" de la: 9\n",
      "y de la: 9\n",
      "organizada por la: 8\n",
      "del Líbano (: 8\n",
      "agentes de la: 8\n",
      "el diario \": 8\n",
      "Central Europeo (: 8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def analyze_org_preceding_trigrams(corpus: List[List[Tuple[str, str, str]]]) -> Counter:\n",
    "    \"\"\"Almacenar trigramas que preceden a organizaciones\"\"\"\n",
    "\n",
    "    trigrams_before_org = []\n",
    "    \n",
    "    for sentence in corpus:\n",
    "        for i in range(len(sentence)):\n",
    "            _, _, ner_tag = sentence[i]\n",
    "            \n",
    "            # Si es el inicio de una organización (B-ORG)\n",
    "            if ner_tag == 'B-ORG':\n",
    "                # Verificar si hay al menos 3 palabras antes\n",
    "                if i >= 3:\n",
    "                    # Extraer las 3 palabras anteriores en el orden correcto\n",
    "                    trigram = (\n",
    "                        sentence[i-3][0],\n",
    "                        sentence[i-2][0],\n",
    "                        sentence[i-1][0]\n",
    "                    )\n",
    "                    trigrams_before_org.append(trigram)\n",
    "    \n",
    "    # Contar frecuencias de trigramas\n",
    "    trigram_counts = Counter(trigrams_before_org)\n",
    "    return trigram_counts\n",
    "\n",
    "# Ejecutar el análisis\n",
    "trigram_counts = analyze_org_preceding_trigrams(train)\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(\"Trigramas de palabras más comunes que preceden a organizaciones:\")\n",
    "print(\"=\" * 70)\n",
    "for trigram, count in trigram_counts.most_common(30):\n",
    "    print(f\"{trigram[0]} {trigram[1]} {trigram[2]}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b115a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedFeatFunc:\n",
    "    def __init__(self, use_Basic: bool = True, use_context_words: bool = True, use_contex_POS_tag: bool = True, use_specific_caracteristics: bool = True, use_lemas: bool = True):\n",
    "        \"\"\"\n",
    "        Constructor de la clase de las funciones de características para el CRFTagger.\n",
    "        Uso:\n",
    "        - use_Basic: Si se deben usar características básicas (longitud, mayúsculas, etc.)\n",
    "        - use_context_words: Si se deben usar palabras de contexto (palabra anterior y siguiente)\n",
    "        - use_contex_POS_tag: Si se deben usar etiquetas POS de contexto (etiqueta anterior y siguiente)\n",
    "        - use_specific_caracteristics: Si se deben usar características específicas (sufijos de ubicación, precedentes de organización)\n",
    "        - use_lemas: Si se deben usar lemas (forma base de la palabra)\n",
    "       \n",
    "        \"\"\"\n",
    "        self.use_basic_features = use_Basic\n",
    "        self.use_context = use_context_words\n",
    "        self.use_conext_POS_tags = use_contex_POS_tag\n",
    "        self.use_specific_caracteristics = use_specific_caracteristics\n",
    "        self.use_lema = use_lemas\n",
    "        \n",
    "        \n",
    "        self.loc_suffixes = {'ía', 'cia', 'dor', 'dal', 'guay', 'cha', 'nia', 'oz'}\n",
    "        \n",
    "        self.common_org_precedents = {\n",
    "            \"presidente de la\",\n",
    "            \"El presidente del\"\n",
    "            \"el presidente del\",\n",
    "            \"el presidente de\",\n",
    "            \"El presidente de\"\n",
    "            \"el portavoz del\",\n",
    "            \"El portavoz del\",\n",
    "            \"general de la\",\n",
    "            \"fuentes de la\",\n",
    "            \"director general de\",\n",
    "            'miembros de la',\n",
    "            'representantes de la',\n",
    "            'secretario general de'\n",
    "        }\n",
    "        \n",
    "        # Cache para resultados\n",
    "        self.cache = {}\n",
    "        \n",
    "\n",
    "    def __call__(self, tokens: list, idx: int) -> dict:\n",
    "        # Obtener la clave única para la oración actual\n",
    "        sentence_key = tuple(tokens) # se convierte a tupla porque las tuplas son inmutables\n",
    "        \n",
    "        # Clave única para caché\n",
    "        cache_key = (sentence_key, idx)\n",
    "        \n",
    "        # Verificar si ya calculamos este caso\n",
    "        if cache_key in self.cache:\n",
    "            return self.cache[cache_key]\n",
    "        \n",
    "        feats = {}\n",
    "        \n",
    "        if idx >= len(tokens) or idx < 0:\n",
    "            self.cache[cache_key] = feats\n",
    "            return feats\n",
    "            \n",
    "        word = tokens[idx][0]\n",
    "\n",
    "        if self.use_basic_features:\n",
    "            # Características básicas\n",
    "            feats[\"word\"] = word\n",
    "            feats[\"length\"] = len(word)\n",
    "            \n",
    "            # Características calculadas una sola vez\n",
    "            if any(char.isdigit() for char in word):\n",
    "                feats[\"has_number\"] = True\n",
    "                \n",
    "            if word and word[0].isupper():\n",
    "                feats[\"is_capitalized\"] = True\n",
    "                \n",
    "            if any(c in string.punctuation for c in word):\n",
    "                feats[\"punctuated\"] = True\n",
    "                \n",
    "            if len(word) > 1:\n",
    "                feats[\"suffix\"] = word[-2:]\n",
    "                feats['prefix'] = word[:2]\n",
    "                \n",
    "            if word and word.isupper():\n",
    "                feats['all_capital'] = True\n",
    "\n",
    "        if self.use_context:\n",
    "            feats[\"word\"] = word\n",
    "            if idx >= 1 and idx < len(tokens)-1:\n",
    "                feats[\"prev_word\"] = tokens[idx-1][0]\n",
    "                feats[\"next_word\"] = tokens[idx+1][0]\n",
    "\n",
    "\n",
    "        if self.use_conext_POS_tags:\n",
    "            \n",
    "            pos = tokens[idx][1]\n",
    "            feats[\"POS\"] = pos\n",
    "\n",
    "            if idx >= 1 and idx < len(tokens)-1:\n",
    "                \n",
    "                # Crear trigramas eficientemente\n",
    "                prev_pos = tokens[idx-1][1]\n",
    "                next_pos = tokens[idx+1][1]\n",
    "                \n",
    "                prev_prev_pos = tokens[idx-2][1] if idx >= 2 else 'START'\n",
    "                next_next_pos = tokens[idx+2][1] if idx < len(tokens)-2 else 'END'\n",
    "                \n",
    "                feats[\"Prev_POS_2\"] = prev_prev_pos\n",
    "                feats[\"Prev_POS_1\"] = prev_pos\n",
    "                feats[\"next_POS_1\"] = next_pos\n",
    "                feats[\"next_pos_2\"] = next_next_pos\n",
    "                \n",
    "        if self.use_lema:\n",
    "           \n",
    "            lem = tokens[idx][2]\n",
    "            feats[\"lema\"] = lem\n",
    "            \n",
    "            \n",
    "        if self.use_specific_caracteristics:\n",
    "            # Verificar sufijos de ubicación\n",
    "            if len(word) > 2:\n",
    "                for suffix in self.loc_suffixes:\n",
    "                    if word.endswith(suffix):\n",
    "                        feats[\"location_suffix\"] = True\n",
    "                        break\n",
    "\n",
    "            # Verificar precedentes de organización\n",
    "            if idx >= 3:\n",
    "                word1 = tokens[idx-3][0]\n",
    "                word2 = tokens[idx-2][0]\n",
    "                word3 = tokens[idx-1][0]\n",
    "\n",
    "                if word1.isdigit() and word2.isalpha() and word3 == \"(\":\n",
    "                    feats[\"organization_precedent\"] = True\n",
    "                else:\n",
    "                    precedent = f\"{word1} {word2} {word3}\".lower()\n",
    "                    if precedent in self.common_org_precedents:\n",
    "                        feats[\"organization_precedent\"] = True\n",
    "\n",
    "        # Guardar en caché\n",
    "        self.cache[cache_key] = feats\n",
    "        return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4eb782d",
   "metadata": {},
   "source": [
    "## 4. Prepare Data for CRF Model\n",
    "\n",
    "Now let's prepare our data for the CRF model, including lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e34d3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "[('Melbourne', 'NP', 'B-LOC'), ('(', 'Fpa', 'O'), ('Australia', 'NP', 'B-LOC'), (')', 'Fpt', 'O'), (',', 'Fc', 'O'), ('25', 'Z', 'O'), ('may', 'NC', 'O'), ('(', 'Fpa', 'O'), ('EFE', 'NC', 'B-ORG'), (')', 'Fpt', 'O'), ('.', 'Fp', 'O')]\n",
      "\n",
      "Processed (with lemmas):\n",
      "[(('Melbourne', 'NP', 'Melbourne'), 'B-LOC'), (('(', 'Fpa', '('), 'O'), (('Australia', 'NP', 'Australia'), 'B-LOC'), ((')', 'Fpt', ')'), 'O'), ((',', 'Fc', ','), 'O'), (('25', 'Z', '25'), 'O'), (('may', 'NC', 'may'), 'O'), (('(', 'Fpa', '('), 'O'), (('EFE', 'NC', 'EFE'), 'B-ORG'), ((')', 'Fpt', ')'), 'O'), (('.', 'Fp', '.'), 'O')]\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "# Load SpaCy model for Spanish\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "def prepare_data_for_crf(conll_data: List[List[Tuple[str, str, str]]], include_lemmas: bool = True) -> List[List[Tuple[Tuple[str, str, str], str]]]:\n",
    "    \"\"\"Process conll data into format for CRF tagging with optional lemmatization.\n",
    "    \n",
    "    Args:\n",
    "        conll_data: List of sentences, where each sentence is a list of tuples (word, pos, tag).\n",
    "        include_lemmas: Whether to include lemmatization in the processed data.\n",
    "        \n",
    "    Returns:\n",
    "        Processed data in the format required for CRF tagging, that is where each tuple has two arguments.\n",
    "        In the first argument, the word, pos and lemma (optional) are included, and in the second argument the tag.\n",
    "    \"\"\"\n",
    "    processed_data = []\n",
    "    \n",
    "    for sentence in conll_data:\n",
    "        # Process entire sentence for better lemmatization context\n",
    "        if include_lemmas:\n",
    "            text = \" \".join(word for word, _, _ in sentence)#works better using the whole sentence (Spacy needs context)\n",
    "            doc = nlp(text) \n",
    "            \n",
    "            # Create processed sentence\n",
    "            processed_sentence = []\n",
    "            for i, (word, pos, tag) in enumerate(sentence):\n",
    "                if i < len(doc):\n",
    "                    lemma = doc[i].lemma_\n",
    "                    processed_sentence.append(((word, pos, lemma), tag))\n",
    "                else:\n",
    "                    # Fallback in case of token mismatch\n",
    "                    processed_sentence.append(((word, pos, word), tag))\n",
    "        else:\n",
    "            processed_sentence = [((word, pos), tag) for word, pos, tag in sentence]\n",
    "            \n",
    "        processed_data.append(processed_sentence)\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "#process the training data with lemmatization \n",
    "processed_train = prepare_data_for_crf(train, include_lemmas=True)\n",
    "\n",
    "# Show the first processed sentence\n",
    "print(\"Original:\")\n",
    "print(train[0])\n",
    "print(\"\\nProcessed (with lemmas):\")\n",
    "print(processed_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb9cca0",
   "metadata": {},
   "source": [
    "Es importante destacar que el usuario debe tener en cuenta que los argumentos de la lematización en el preprocesamiento del texto (\"include_lemmas\") y en la instancia de la clase OptimizedFeatFunc (\"use_lemas\") deben de ser los mismos (los dos a True o los dos a False) para el correcto funcionamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb323ce9",
   "metadata": {},
   "source": [
    "## 5. Train a CRF Model for NER\n",
    "\n",
    "Let's train a small CRF model using our feature function and prepared data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abde6cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRF model trained!\n"
     ]
    }
   ],
   "source": [
    "# Create our feature function\n",
    "feature_func = OptimizedFeatFunc(True,True, True, True, True)\n",
    "\n",
    "# Initialize CRF tagger with our feature function\n",
    "crf_tagger = CRFTagger(feature_func=feature_func)\n",
    "\n",
    "# For demonstration, train on a small subset\n",
    "crf_tagger.train(processed_train, 'example_model.crf.tagger')\n",
    "\n",
    "print(\"CRF model trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2517cd",
   "metadata": {},
   "source": [
    "## 6. Experiment with Different Tag Encoding Schemes\n",
    "\n",
    "The BIO scheme is the most common, but let's implement functions to convert to other schemes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "789fa5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original (BIO):\n",
      "[('Por', 'SP', 'O'), ('su', 'DP', 'O'), ('parte', 'NC', 'O'), (',', 'Fc', 'O'), ('el', 'DA', 'O'), ('Abogado', 'NC', 'B-PER'), ('General', 'AQ', 'I-PER'), ('de', 'SP', 'O'), ('Victoria', 'NC', 'B-LOC'), (',', 'Fc', 'O'), ('Rob', 'NC', 'B-PER'), ('Hulls', 'AQ', 'I-PER'), (',', 'Fc', 'O'), ('indicó', 'VMI', 'O'), ('que', 'CS', 'O'), ('no', 'RN', 'O'), ('hay', 'VAI', 'O'), ('nadie', 'PI', 'O'), ('que', 'PR', 'O'), ('controle', 'VMS', 'O'), ('que', 'CS', 'O'), ('las', 'DA', 'O'), ('informaciones', 'NC', 'O'), ('contenidas', 'AQ', 'O'), ('en', 'SP', 'O'), ('CrimeNet', 'NC', 'B-MISC'), ('son', 'VSI', 'O'), ('veraces', 'AQ', 'O'), ('.', 'Fp', 'O')]\n",
      "\n",
      "IO Scheme:\n",
      "[('Por', 'SP', 'O'), ('su', 'DP', 'O'), ('parte', 'NC', 'O'), (',', 'Fc', 'O'), ('el', 'DA', 'O'), ('Abogado', 'NC', 'I-PER'), ('General', 'AQ', 'I-PER'), ('de', 'SP', 'O'), ('Victoria', 'NC', 'I-LOC'), (',', 'Fc', 'O'), ('Rob', 'NC', 'I-PER'), ('Hulls', 'AQ', 'I-PER'), (',', 'Fc', 'O'), ('indicó', 'VMI', 'O'), ('que', 'CS', 'O'), ('no', 'RN', 'O'), ('hay', 'VAI', 'O'), ('nadie', 'PI', 'O'), ('que', 'PR', 'O'), ('controle', 'VMS', 'O'), ('que', 'CS', 'O'), ('las', 'DA', 'O'), ('informaciones', 'NC', 'O'), ('contenidas', 'AQ', 'O'), ('en', 'SP', 'O'), ('CrimeNet', 'NC', 'I-MISC'), ('son', 'VSI', 'O'), ('veraces', 'AQ', 'O'), ('.', 'Fp', 'O')]\n",
      "\n",
      "BIOES Scheme:\n",
      "[('Por', 'SP', 'O'), ('su', 'DP', 'O'), ('parte', 'NC', 'O'), (',', 'Fc', 'O'), ('el', 'DA', 'O'), ('Abogado', 'NC', 'B-PER'), ('General', 'AQ', 'E-PER'), ('de', 'SP', 'O'), ('Victoria', 'NC', 'S-LOC'), (',', 'Fc', 'O'), ('Rob', 'NC', 'B-PER'), ('Hulls', 'AQ', 'E-PER'), (',', 'Fc', 'O'), ('indicó', 'VMI', 'O'), ('que', 'CS', 'O'), ('no', 'RN', 'O'), ('hay', 'VAI', 'O'), ('nadie', 'PI', 'O'), ('que', 'PR', 'O'), ('controle', 'VMS', 'O'), ('que', 'CS', 'O'), ('las', 'DA', 'O'), ('informaciones', 'NC', 'O'), ('contenidas', 'AQ', 'O'), ('en', 'SP', 'O'), ('CrimeNet', 'NC', 'S-MISC'), ('son', 'VSI', 'O'), ('veraces', 'AQ', 'O'), ('.', 'Fp', 'O')]\n"
     ]
    }
   ],
   "source": [
    "def bio_to_io(tagged_sent: List[Tuple[str, str, str]]) -> List[Tuple[str, str, str]]:\n",
    "    \"\"\"Convert BIO tagging to IO tagging\n",
    "    \n",
    "    Args:\n",
    "        tagged_sent: List of tuples where each tuple contains (word, pos, tag) in BIO format.\n",
    "        \n",
    "    Returns:\n",
    "        List of tuples where each tuple contains (word, pos, tag) in IO format.\n",
    "    \"\"\"\n",
    "    io_sent = []\n",
    "    for word, pos, tag in tagged_sent:\n",
    "        if tag == \"O\":\n",
    "            io_sent.append((word, pos, tag))\n",
    "        else:\n",
    "            # Replace B- with I- for any entity tag\n",
    "            entity_type = tag[2:]\n",
    "            io_sent.append((word, pos, f\"I-{entity_type}\"))\n",
    "    return io_sent\n",
    "\n",
    "def bio_to_bioes(sent: List[Tuple[str, str, str]]) -> List[Tuple[str, str, str]]:\n",
    "    \"\"\"Convert BIO tagging to BIOES tagging\n",
    "    Args:\n",
    "        tagged_sent: List of tuples where each tuple contains (word, pos, tag) in BIO format.\n",
    "        \"\"\"\n",
    "    new_sent = []\n",
    "    n = len(sent)\n",
    "    i = 0\n",
    "    \n",
    "    while i < n:\n",
    "        word, pos, tag = sent[i]\n",
    "        \n",
    "        if tag == \"O\":\n",
    "            new_sent.append((word, pos, tag))\n",
    "            i += 1\n",
    "        elif tag.startswith(\"B-\"):\n",
    "            entity_type = tag[2:]\n",
    "            \n",
    "            # Check if it's a singleton entity (no following I- tags)\n",
    "            if i + 1 == n or not sent[i+1][2].startswith(f\"I-{entity_type}\"):\n",
    "                new_sent.append((word, pos, f\"S-{entity_type}\"))\n",
    "                i += 1\n",
    "            else:\n",
    "                # It's the beginning of a multi-token entity\n",
    "                new_sent.append((word, pos, f\"B-{entity_type}\"))\n",
    "                i += 1\n",
    "                \n",
    "                # Process all the intermediate I- tags\n",
    "                while i < n and sent[i][2] == f\"I-{entity_type}\":\n",
    "                    # Check if this is the last I- tag\n",
    "                    if i + 1 == n or sent[i+1][2] != f\"I-{entity_type}\":\n",
    "                        new_sent.append((sent[i][0], sent[i][1], f\"E-{entity_type}\"))\n",
    "                    else:\n",
    "                        new_sent.append((sent[i][0], sent[i][1], f\"I-{entity_type}\"))\n",
    "                    i += 1\n",
    "        else:\n",
    "            # Handle unexpected tags (like I- without preceding B-)\n",
    "            new_sent.append((word, pos, tag))\n",
    "            i += 1\n",
    "            \n",
    "    return new_sent\n",
    "\n",
    "def bio_to_bioe(sent: List[Tuple[str, str, str]]) -> List[Tuple[str, str, str]]:\n",
    "    \"\"\"Convert BIO tagging to BIOE tagging (BIOE includes E to indicate the end of an entity)\n",
    "    Args:\n",
    "        tagged_sent: List of tuples where each tuple contains (word, pos, tag) in BIO format.\n",
    "        \"\"\"\n",
    "    new_sent = []\n",
    "    n = len(sent)\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        word, pos, tag = sent[i]\n",
    "        entity_type = tag[2:]\n",
    "      \n",
    "        if tag[:2] == 'B-':\n",
    "            new_sent.append((word, pos, f'B-{entity_type}'))\n",
    "            i += 1\n",
    "            while i < n and sent[i][2] == f'I-{entity_type}':\n",
    "                # Is it the last I-?\n",
    "                if i + 1 == n or sent[i+1][2] != f'I-{entity_type}':\n",
    "                    new_sent.append((sent[i][0], sent[i][1], f'E-{entity_type}'))\n",
    "                else:\n",
    "                    new_sent.append((sent[i][0], sent[i][1], f'I-{entity_type}'))\n",
    "                i += 1\n",
    "        else:\n",
    "            new_sent.append((word, pos, tag))\n",
    "            i += 1\n",
    "    return new_sent\n",
    "\n",
    "def bio_to_biow(sent: List[Tuple[str, str, str]]) -> List[Tuple[str, str, str]]:\n",
    "    \"\"\"Convert BIO tagging to BIOW tagging (BIOW includes W to indicate single-token entities)\n",
    "    Args:\n",
    "        tagged_sent: List of tuples where each tuple contains (word, pos, tag) in BIO format.\n",
    "        \"\"\"\n",
    "    new_sent = []\n",
    "    n = len(sent)\n",
    "    for i, (word, pos, chunk) in enumerate(sent):\n",
    "        if chunk[:2] == \"B-\":\n",
    "            if ((i+1) == n) or (not sent[i+1][2][:2] == \"I-\"):\n",
    "                new_chunk = \"W-\" + chunk[2:]\n",
    "            else:\n",
    "                new_chunk = chunk\n",
    "        else:\n",
    "            new_chunk = chunk\n",
    "        new_sent.append((word, pos, new_chunk))\n",
    "    return new_sent\n",
    "\n",
    "def bio_to_bioes(sent: List[Tuple[str, str, str]]) -> List[Tuple[str, str, str]]:\n",
    "    \"\"\"Convert BIO tagging to BIOES tagging (BIOES includes E to indicate the end of an entity and S for single-token entities)\n",
    "    Args:\n",
    "        tagged_sent: List of tuples where each tuple contains (word, pos, tag) in BIO format.\n",
    "        \"\"\"\n",
    "    new_sent = []\n",
    "    n = len(sent)\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        word, pos, tag = sent[i]\n",
    "        entity_type = tag[2:]\n",
    "      \n",
    "        if tag[:2] == 'B-':\n",
    "            # Check if it is a single-token entity\n",
    "            if i + 1 == n or sent[i+1][2][:2] != \"I-\":\n",
    "                new_sent.append((word, pos, f'S-{entity_type}'))\n",
    "                i += 1\n",
    "            else:\n",
    "                new_sent.append((word, pos, f'B-{entity_type}'))\n",
    "                i += 1\n",
    "                while i < n and sent[i][2] == f'I-{entity_type}':\n",
    "                    # Is it the last I-?\n",
    "                    if i + 1 == n or sent[i+1][2] != f'I-{entity_type}':\n",
    "                        new_sent.append((sent[i][0], sent[i][1], f'E-{entity_type}'))\n",
    "                    else:\n",
    "                        new_sent.append((sent[i][0], sent[i][1], f'I-{entity_type}'))\n",
    "                    i += 1\n",
    "        else:\n",
    "            new_sent.append((word, pos, tag))\n",
    "            i += 1\n",
    "    return new_sent\n",
    "\n",
    "# Example of converting tags\n",
    "sample_sent = train[5]  # Get a sentence that hopefully has some entities\n",
    "print(\"Original (BIO):\")\n",
    "print(sample_sent)\n",
    "\n",
    "io_sent = bio_to_io(sample_sent)\n",
    "print(\"\\nIO Scheme:\")\n",
    "print(io_sent)\n",
    "\n",
    "bioes_sent = bio_to_bioes(sample_sent)\n",
    "print(\"\\nBIOES Scheme:\")\n",
    "print(bioes_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9e0e53",
   "metadata": {},
   "source": [
    "## 7. Entity-Level Evaluation\n",
    "\n",
    "Instead of just token-level accuracy, let's implement entity-level evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9030cda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(tags):\n",
    "    \"\"\"\n",
    "    Extract entity spans from a sequence of BIO tags.\n",
    "    \n",
    "    Args:\n",
    "        tags: List of BIO tags (e.g., 'B-PER', 'I-PER', 'O')\n",
    "        \n",
    "    Returns:\n",
    "        List of tuples (entity_type, start_idx, end_idx)\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    entity_type = None\n",
    "    start_idx = None\n",
    "    \n",
    "    for i, tag in enumerate(tags):\n",
    "        # Handle the case where tag might be a tuple\n",
    "        if isinstance(tag, tuple):\n",
    "            tag = tag[1]  # Extract the actual tag if it's a tuple (word, tag)\n",
    "            \n",
    "        if tag.startswith('B-'):\n",
    "            # If we were tracking an entity, add it to the list\n",
    "            if entity_type is not None:\n",
    "                entities.append((entity_type, start_idx, i - 1))\n",
    "            # Start a new entity\n",
    "            entity_type = tag[2:]  # Remove 'B-' prefix\n",
    "            start_idx = i\n",
    "        elif tag.startswith('I-'):\n",
    "            # Continue with the current entity\n",
    "            curr_type = tag[2:]  # Remove 'I-' prefix\n",
    "            # This handles inconsistent I- tags that don't match the current entity\n",
    "            if entity_type is None or curr_type != entity_type:\n",
    "                # Close any open entity and ignore this tag (it's an error in tagging)\n",
    "                if entity_type is not None:\n",
    "                    entities.append((entity_type, start_idx, i - 1))\n",
    "                entity_type = None\n",
    "                start_idx = None\n",
    "        else:  # 'O' tag\n",
    "            # If we were tracking an entity, add it to the list\n",
    "            if entity_type is not None:\n",
    "                entities.append((entity_type, start_idx, i - 1))\n",
    "                entity_type = None\n",
    "                start_idx = None\n",
    "    \n",
    "    # Don't forget the last entity if the sequence ends with an entity\n",
    "    if entity_type is not None:\n",
    "        entities.append((entity_type, start_idx, len(tags) - 1))\n",
    "    \n",
    "    return entities\n",
    "\n",
    "\n",
    "def evaluate_entities(gold_entities, pred_entities):\n",
    "    \"\"\"\n",
    "    Calculate precision, recall, and F1 score for entity recognition.\n",
    "    \n",
    "    Args:\n",
    "        gold_entities: List of gold standard entity tuples (type, start, end)\n",
    "        pred_entities: List of predicted entity tuples (type, start, end)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with precision, recall, and F1 scores\n",
    "    \"\"\"\n",
    "    # Convert to sets for easier comparison\n",
    "    gold_set = set(gold_entities)\n",
    "    pred_set = set(pred_entities)\n",
    "    \n",
    "    # Calculate correct predictions (intersection)\n",
    "    correct = len(gold_set.intersection(pred_set))\n",
    "    \n",
    "    # Calculate precision, recall, and F1\n",
    "    precision = correct / len(pred_set) if pred_set else 0.0\n",
    "    recall = correct / len(gold_set) if gold_set else 1.0  # Perfect recall if no gold entities\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'gold_count': len(gold_set),\n",
    "        'pred_count': len(pred_set),\n",
    "        'correct': correct\n",
    "    }\n",
    "\n",
    "def evaluate_ner_corpus(gold_data, predicted_data):\n",
    "    \"\"\"\n",
    "    Evaluate NER performance at entity level across an entire corpus.\n",
    "    \n",
    "    Args:\n",
    "        gold_data: List of sentences where each sentence is a list of (tupla, gold_tag) tuples\n",
    "        predicted_data: List of sentences where each sentence is a list of (word, pred_tag) tuples\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with overall precision, recall, and F1 scores\n",
    "    \"\"\"\n",
    "    total_correct = 0\n",
    "    total_gold = 0\n",
    "    total_pred = 0\n",
    "    \n",
    "    \n",
    "    for i in range(len(gold_data)):\n",
    "        # Extract just the tags\n",
    "        sentence = gold_data[i]\n",
    "        sentence_pred = predicted_data[i]\n",
    "        \n",
    "        gold_tags = []\n",
    "        pred_tags = []\n",
    "        for j in range(len(sentence)):\n",
    "           \n",
    "            gold_tags.append(sentence[j][1])\n",
    "            pred_tags.append(sentence_pred[j][1])\n",
    "           \n",
    "        # Extract entities\n",
    "        gold_entities = extract_entities(gold_tags)\n",
    "        pred_entities = extract_entities(pred_tags)\n",
    "        \n",
    "        # Evaluate this sentence\n",
    "        results = evaluate_entities(gold_entities, pred_entities)\n",
    "        \n",
    "        # Accumulate counts\n",
    "        total_correct += results['correct']\n",
    "        total_gold += results['gold_count']\n",
    "        total_pred += results['pred_count']\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    precision = total_correct / total_pred if total_pred > 0 else 0.0\n",
    "    recall = total_correct / total_gold if total_gold > 0 else 1.0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    accuracy = total_correct / total_gold\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'accuracy': accuracy  # Using F1 as the \"accuracy\" metric for entity-level evaluation\n",
    "    }\n",
    "\n",
    "\n",
    "# Extend CRFTagger to support entity-level evaluation\n",
    "def entity_level_accuracy(tagger, test_data):\n",
    "    \"\"\"\n",
    "    Calculate entity-level evaluation metrics for a CRFTagger.\n",
    "    \n",
    "    Args:\n",
    "        tagger: Trained CRFTagger model\n",
    "        test_data: List of sentences where each sentence is a list of ((word, pos, lema), tag) tuples\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with precision, recall, F1, and accuracy scores\n",
    "    \"\"\"\n",
    "    # Convert test data to the format expected by the evaluation function\n",
    "    \n",
    "    # Get predictions\n",
    "    predicted_data = []\n",
    "    for sentence in test_data:  # Use original test_data to extract words\n",
    "        words = [tupla for tupla, _ in sentence]\n",
    "        tags = tagger.tag(words)\n",
    "        predicted_data.append(list(zip(words, tags)))\n",
    "    \n",
    "    # Evaluate\n",
    "    results = evaluate_ner_corpus(test_data, predicted_data)\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b8de371",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_test = prepare_data_for_crf(test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab3d9a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity-Level Evaluation:\n",
      "Precision: 0.7806\n",
      "Recall: 0.7650\n",
      "F1 Score: 0.7727\n",
      "\n",
      "Token-Level Accuracy: 0.9702\n"
     ]
    }
   ],
   "source": [
    "# Replace the token-level accuracy evaluation with entity-level evaluation\n",
    "entity_metrics = entity_level_accuracy(crf_tagger, preprocessed_test)  # Changed from trained_tagger to crf_tagger\n",
    "\n",
    "print(\"Entity-Level Evaluation:\")\n",
    "print(f\"Precision: {entity_metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {entity_metrics['recall']:.4f}\")\n",
    "print(f\"F1 Score: {entity_metrics['f1']:.4f}\")\n",
    "\n",
    "# For comparison, also show the token-level accuracy\n",
    "# First prepare test data in the format expected by the CRF tagger\n",
    "token_accuracy = crf_tagger.accuracy(preprocessed_test)  # Changed from trained_tagger to crf_tagger\n",
    "print(f\"\\nToken-Level Accuracy: {token_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8872659",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "This notebook has demonstrated:\n",
    "\n",
    "1. How to use the `data` class for processing NER data\n",
    "2. How to implement and customize a feature function\n",
    "3. How to prepare data for CRF tagging\n",
    "4. How to convert between different tagging schemes (BIO, IO, BIOES)\n",
    "5. How to perform entity-level evaluation\n",
    "\n",
    "These techniques can be applied to improve NER performance for Spanish and other languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02054101",
   "metadata": {},
   "source": [
    "## 9. Feature Combination Analysis\n",
    "\n",
    "Let's evaluate different combinations of features to find the optimal configuration using entity-level evaluation metrics instead of token-level accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c88fc81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_completo():\n",
    "    # Define feature groups to test\n",
    "    feature_groups = {\n",
    "        \"Basic\": True,        # word, length, etc.\n",
    "        \"Context_Words\": True,  # prev_word, next_word\n",
    "        \"Context_POS\": True,    # POS tags of surrounding words\n",
    "        \"Specific\": True,      # location_suffix, organization_precedent, etc.\n",
    "        \"Lemmas\": True,        # Use lemmatization features\n",
    "    }\n",
    "    \n",
    "    # Initialize variables to track best configurations\n",
    "    best_features = []\n",
    "    best_scores = []\n",
    "    best_models = []\n",
    "    \n",
    "    print(\"Starting greedy feature selection...\")\n",
    "    \n",
    "    # Iterate through feature counts (1 to n)\n",
    "    for r in range(1, len(feature_groups) + 1):\n",
    "        print(f\"\\n--- Finding best configuration with {r} features ---\")\n",
    "        \n",
    "        # In first round, test all individual features\n",
    "        # In subsequent rounds, only test combinations that include previous best features\n",
    "        if r == 1:\n",
    "            # Test each feature individually\n",
    "            candidates = [[feat] for feat in feature_groups.keys()]\n",
    "        else:\n",
    "            # Keep best features from previous round and test adding one more\n",
    "            candidates = [best_features[-1] + [feat] for feat in feature_groups.keys() \n",
    "                         if feat not in best_features[-1]]\n",
    "        \n",
    "        best_config = None\n",
    "        best_score = 0\n",
    "        best_model = None\n",
    "        \n",
    "        # Test all candidate configurations for this round\n",
    "        for candidate_features in candidates:\n",
    "            # Create configuration with only these features enabled\n",
    "            config = {feat: (feat in candidate_features) for feat in feature_groups.keys()}\n",
    "            print(f\"Testing configuration: {config} out of {len(candidates)}\")\n",
    "            \n",
    "            # Test this configuration\n",
    "            start_time = time.time()\n",
    "            tagger, entity_metrics = evaluate_feature_combination(config)\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            # Check if this is the best so far\n",
    "            f1_score = entity_metrics['f1']\n",
    "            print(f\"F1 Score: {f1_score:.4f}, Time: {elapsed:.2f} seconds\")\n",
    "            \n",
    "            if f1_score > best_score:\n",
    "                best_score = f1_score\n",
    "                best_config = candidate_features\n",
    "                best_model = tagger\n",
    "        \n",
    "        # Save best configuration for this round\n",
    "        best_features.append(best_config)\n",
    "        best_scores.append(best_score)\n",
    "        best_models.append(best_model)\n",
    "        \n",
    "        # Save the best model for this round\n",
    "        if best_model:\n",
    "            # Generate a descriptive model name with round number and features\n",
    "            features_str = '_'.join([k[:1] for k, v in {feat: True for feat in best_config}.items()])\n",
    "            model_path = f'best_model_r{r}_{features_str}.crf.tagger'\n",
    "            \n",
    "            # Train and save the best model for this round\n",
    "            best_model.train(processed_train, model_path)\n",
    "            print(f\"Saved best model with {r} features to {model_path}\")\n",
    "        \n",
    "        print(f\"Best configuration with {r} features: {best_config}\")\n",
    "        print(f\"Best F1 score: {best_score:.4f}\")\n",
    "    \n",
    "    # Print summary of all best configurations\n",
    "    print(\"\\n=== Summary of Greedy Feature Selection ===\")\n",
    "    for r in range(len(best_features)):\n",
    "        print(f\"Round {r+1}: Best features = {best_features[r]}, F1 score = {best_scores[r]:.4f}\")\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(best_scores)+1), best_scores, marker='o')\n",
    "    plt.xlabel('Number of Features')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('F1 Score vs. Number of Features (Greedy Selection)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return best_features, best_scores, best_models\n",
    "\n",
    "def evaluate_feature_combination(config):\n",
    "    \"\"\"Evaluate a single feature configuration and return metrics\"\"\"\n",
    "    # Create feature function with the specified configuration\n",
    "    feat_func = OptimizedFeatFunc(\n",
    "        use_Basic=config['Basic'], \n",
    "        use_context_words=config['Context_Words'], \n",
    "        use_contex_POS_tag=config['Context_POS'], \n",
    "        use_specific_caracteristics=config['Specific'],\n",
    "        use_lemas=config['Lemmas']\n",
    "    )\n",
    "    \n",
    "    # Create the CRF tagger with the feature function\n",
    "    ct = CRFTagger(feature_func=feat_func)\n",
    "    \n",
    "    # For faster testing during development, use a subset of the data\n",
    "    train_sample = processed_train\n",
    "    \n",
    "    \n",
    "    # Create a temporary model file for evaluation\n",
    "    ct.train(train_sample, 'temp_model.crf.tagger')\n",
    "    \n",
    "    # Calculate entity-level metrics\n",
    "    entity_metrics = entity_level_accuracy(ct, preprocessed_test)\n",
    "    \n",
    "    return ct, entity_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5bb2c1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting greedy feature selection...\n",
      "\n",
      "--- Finding best configuration with 1 features ---\n",
      "Testing configuration: {'Basic': True, 'Context_Words': False, 'Context_POS': False, 'Specific': False, 'Lemmas': False} out of 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# This will take some time to run - only saves the best model per round\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain_completo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 44\u001b[0m, in \u001b[0;36mtrain_completo\u001b[1;34m()\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Test this configuration\u001b[39;00m\n\u001b[0;32m     43\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 44\u001b[0m tagger, entity_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_feature_combination\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Check if this is the best so far\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[19], line 110\u001b[0m, in \u001b[0;36mevaluate_feature_combination\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    106\u001b[0m train_sample \u001b[38;5;241m=\u001b[39m processed_train\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# Create a temporary model file for evaluation\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m \u001b[43mct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtemp_model.crf.tagger\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# Calculate entity-level metrics\u001b[39;00m\n\u001b[0;32m    113\u001b[0m entity_metrics \u001b[38;5;241m=\u001b[39m entity_level_accuracy(ct, preprocessed_test)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nltk\\tag\\crf.py:190\u001b[0m, in \u001b[0;36mCRFTagger.train\u001b[1;34m(self, train_data, model_file)\u001b[0m\n\u001b[0;32m    187\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mappend(features, labels)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# Now train the model, the output should be model_file\u001b[39;00m\n\u001b[1;32m--> 190\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;66;03m# Save the model file\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_model_file(model_file)\n",
      "File \u001b[1;32mc:\\Users\\jiahu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pycrfsuite\\_pycrfsuite.pyx:359\u001b[0m, in \u001b[0;36mpycrfsuite._pycrfsuite.BaseTrainer.train\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\jiahu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pycrfsuite\\_pycrfsuite.pyx:272\u001b[0m, in \u001b[0;36mpycrfsuite._pycrfsuite.BaseTrainer._on_message\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\jiahu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pycrfsuite\\_pycrfsuite.pyx:499\u001b[0m, in \u001b[0;36mpycrfsuite._pycrfsuite.Trainer.message\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\jiahu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pycrfsuite\\_logparser.py:30\u001b[0m, in \u001b[0;36mTrainLogParser.feed\u001b[1;34m(self, line)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevents\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog)))\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 30\u001b[0m event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhandle_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate)(line)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     start, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevents[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m2\u001b[39m], \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This will take some time to run - only saves the best model per round\n",
    "train_completo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69d3e4d",
   "metadata": {},
   "source": [
    "## 10. Full Analysis with Optimized Features\n",
    "\n",
    "Now that we've identified the best feature combinations, we can train a model with the optimal configuration and evaluate it on the full test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f24d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of running a complete analysis with the optimal feature configuration\n",
    "def run_optimal_configuration():\n",
    "    # Create feature function with optimal settings (example - replace with your findings)\n",
    "    optimal_feat_func = OptimizedFeatureFunction(\n",
    "        use_basic=True,\n",
    "        use_context=True, \n",
    "        use_pos=True,\n",
    "        use_lemmas=True,  # Including lemmatization\n",
    "        use_specific=True\n",
    "    )\n",
    "    \n",
    "    print(\"Processing training data...\")\n",
    "    # Process the training data\n",
    "    processed_train = []\n",
    "    for i, sentence in enumerate(train):\n",
    "        if i % 500 == 0:  # Progress indicator\n",
    "            print(f\"Processing sentence {i}/{len(train)}...\")\n",
    "        \n",
    "        # Use our data class to process the sentence\n",
    "        sentence_processor = data(sentence)\n",
    "        words = sentence_processor.get_word()\n",
    "        pos_tags = sentence_processor.get_pos()\n",
    "        bio_tags = sentence_processor.get_bio()\n",
    "        lemmas = sentence_processor(language=\"es\")  # Get lemmas\n",
    "        \n",
    "        # Format for CRF training\n",
    "        processed_sentence = []\n",
    "        for i, (word, pos, bio) in enumerate(zip(words, pos_tags, bio_tags)):\n",
    "            if i < len(lemmas):\n",
    "                processed_sentence.append(((word, pos, lemmas[i]), bio))\n",
    "            else:\n",
    "                processed_sentence.append(((word, pos, word.lower()), bio))\n",
    "                \n",
    "        processed_train.append(processed_sentence)\n",
    "    \n",
    "    print(\"Training CRF model...\")\n",
    "    # Train the model\n",
    "    optimal_tagger = CRFTagger(feature_func=optimal_feat_func)\n",
    "    optimal_tagger.train(processed_train, 'optimal_model.crf.tagger')\n",
    "    \n",
    "    print(\"Processing test data...\")\n",
    "    # Process the test data\n",
    "    processed_test = []\n",
    "    for sentence in test:\n",
    "        # Use our data class to process the sentence\n",
    "        sentence_processor = data(sentence)\n",
    "        words = sentence_processor.get_word()\n",
    "        pos_tags = sentence_processor.get_pos()\n",
    "        bio_tags = sentence_processor.get_bio()\n",
    "        lemmas = sentence_processor(language=\"es\")  # Get lemmas\n",
    "        \n",
    "        # Format for evaluation\n",
    "        processed_sentence = []\n",
    "        for i, (word, pos, bio) in enumerate(zip(words, pos_tags, bio_tags)):\n",
    "            if i < len(lemmas):\n",
    "                processed_sentence.append(((word, pos, lemmas[i]), bio))\n",
    "            else:\n",
    "                processed_sentence.append(((word, pos, word.lower()), bio))\n",
    "                \n",
    "        processed_test.append(processed_sentence)\n",
    "    \n",
    "    # Evaluate using entity-level metrics\n",
    "    entity_results = entity_level_accuracy(optimal_tagger, test)\n",
    "    \n",
    "    print(\"\\n=== Entity-Level Evaluation Results ===\")\n",
    "    print(f\"Precision: {entity_results['precision']:.4f}\")\n",
    "    print(f\"Recall: {entity_results['recall']:.4f}\")\n",
    "    print(f\"F1 Score: {entity_results['f1']:.4f}\")\n",
    "    \n",
    "    return optimal_tagger, entity_results\n",
    "\n",
    "# Uncomment to run the complete analysis (it will take a significant amount of time)\n",
    "# optimal_tagger, results = run_optimal_configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49987bbc",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated:\n",
    "\n",
    "1. How to use the custom `data` class to process NER data and extract lemmas\n",
    "2. Implementing a feature function class with various customizable feature sets\n",
    "3. Training and evaluating CRF models for NER with different feature combinations\n",
    "4. Converting between different tagging schemes (BIO, IO, BIOES)\n",
    "5. Entity-level evaluation for more accurate performance measurement\n",
    "6. Systematic analysis of feature importance for NER performance\n",
    "\n",
    "The entity-level evaluation provides a more meaningful assessment of NER performance than token-level accuracy, as it ensures that complete entities are correctly identified rather than just individual tokens."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
