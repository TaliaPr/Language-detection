{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e1764a4",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER) Example\n",
    "\n",
    "This notebook demonstrates how to use the data processing classes and NER functionality implemented in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8edf90df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import conll2002\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from nltk.tag import CRFTagger\n",
    "\n",
    "from Pruebas_Hui import data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c587182",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data\n",
    "\n",
    "First, we'll load data from the conll2002 corpus for Spanish NER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8dcce6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 8323 sentences\n",
      "Test set: 1517 sentences\n",
      "\n",
      "Example sentence:\n",
      "[('Melbourne', 'NP', 'B-LOC'), ('(', 'Fpa', 'O'), ('Australia', 'NP', 'B-LOC'), (')', 'Fpt', 'O'), (',', 'Fc', 'O'), ('25', 'Z', 'O'), ('may', 'NC', 'O'), ('(', 'Fpa', 'O'), ('EFE', 'NC', 'B-ORG'), (')', 'Fpt', 'O'), ('.', 'Fp', 'O')]\n"
     ]
    }
   ],
   "source": [
    "# Load the Spanish NER data\n",
    "train = conll2002.iob_sents('esp.train')\n",
    "test = conll2002.iob_sents('esp.testb')\n",
    "\n",
    "print(f\"Training set: {len(train)} sentences\")\n",
    "print(f\"Test set: {len(test)} sentences\")\n",
    "\n",
    "# Show an example sentence\n",
    "print(\"\\nExample sentence:\")\n",
    "print(train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcda8b4d",
   "metadata": {},
   "source": [
    "## 2. Process Data Using Custom Classes\n",
    "\n",
    "Let's use our custom `data` class to process a sample sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af2e400d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: ['Melbourne', '(', 'Australia', ')', ',', '25', 'may', '(', 'EFE', ')', '.', '-', 'El', 'Abogado', 'General', 'del', 'Estado', ',', 'Daryl', 'Williams', ',', 'subrayó', 'hoy', 'la', 'necesidad', 'de', 'tomar', 'medidas', 'para', 'proteger', 'al', 'sistema', 'judicial', 'australiano', 'frente', 'a', 'una', 'página', 'de', 'internet', 'que', 'imposibilita', 'el', 'cumplimiento', 'de', 'los', 'principios', 'básicos', 'de', 'la', 'Ley', '.', 'La', 'petición', 'del', 'Abogado', 'General', 'tiene', 'lugar', 'después', 'de', 'que', 'un', 'juez', 'del', 'Tribunal', 'Supremo', 'del', 'estado', 'de', 'Victoria', '(', 'Australia', ')', 'se', 'viera', 'forzado', 'a', 'disolver', 'un', 'jurado', 'popular', 'y', 'suspender', 'el', 'proceso', 'ante', 'el', 'argumento', 'de', 'la', 'defensa', 'de', 'que', 'las', 'personas', 'que', 'lo', 'componían', 'podían', 'haber', 'obtenido', 'información', 'sobre', 'el', 'acusado', 'a', 'través', 'de', 'la', 'página', 'CrimeNet', '.', 'Esta', 'página', 'web', 'lleva', 'un', 'mes', 'de', 'existencia', ',', 'tiempo', 'en', 'el', 'que', 'ha', 'sido', 'visitada', 'en', 'más', 'de', 'un', 'millón', 'de', 'ocasiones', ',', 'y', 'facilita', 'información', 'sobre', 'miles', 'de', 'crímenes', 'y', 'criminales', 'ya', 'enjuiciados', 'o', 'aún', 'perseguidos', ',', 'datos', 'que', 'salen', 'de', 'artículos', 'de', 'periódicos', 'y', 'archivos', 'judiciales', '.']\n",
      "POS tags: ['NP', 'Fpa', 'NP', 'Fpt', 'Fc', 'Z', 'NC', 'Fpa', 'NC', 'Fpt', 'Fp', 'Fg', 'DA', 'NC', 'AQ', 'SP', 'NC', 'Fc', 'VMI', 'NC', 'Fc', 'VMI', 'RG', 'DA', 'NC', 'SP', 'VMN', 'NC', 'SP', 'VMN', 'SP', 'NC', 'AQ', 'AQ', 'RG', 'SP', 'DI', 'NC', 'SP', 'NC', 'PR', 'VMI', 'DA', 'NC', 'SP', 'DA', 'NC', 'AQ', 'SP', 'DA', 'NC', 'Fp', 'DA', 'NC', 'SP', 'NC', 'AQ', 'VMI', 'NC', 'RG', 'SP', 'CS', 'DI', 'NC', 'SP', 'NC', 'AQ', 'SP', 'NC', 'SP', 'NC', 'Fpa', 'NP', 'Fpt', 'P0', 'VMS', 'AQ', 'SP', 'VMN', 'DI', 'NC', 'AQ', 'CC', 'VMN', 'DA', 'NC', 'SP', 'DA', 'NC', 'SP', 'DA', 'NC', 'SP', 'CS', 'DA', 'NC', 'PR', 'PP', 'VMI', 'VMI', 'VAN', 'VMP', 'NC', 'SP', 'DA', 'VMP', 'SP', 'NC', 'SP', 'DA', 'NC', 'AQ', 'Fp', 'DD', 'NC', 'AQ', 'VMI', 'DI', 'NC', 'SP', 'NC', 'Fc', 'NC', 'SP', 'DA', 'PR', 'VAI', 'VSP', 'VMP', 'SP', 'RG', 'SP', 'DI', 'NC', 'SP', 'NC', 'Fc', 'CC', 'VMI', 'NC', 'SP', 'PN', 'SP', 'NC', 'CC', 'VMM', 'RG', 'VMP', 'CC', 'RG', 'VMM', 'Fc', 'NC', 'PR', 'VMI', 'SP', 'NC', 'SP', 'NC', 'CC', 'NC', 'AQ', 'Fp']\n",
      "BIO tags: ['B-LOC', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'B-LOC', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Lemmas: ['Melbourne', '(', 'Australia', ')', ',', '25', 'may', '(', 'EFE', ')', '.', '-', 'el', 'Abogado', 'General', 'del', 'Estado', ',', 'Daryl', 'Williams', ',', 'subrayar', 'hoy', 'el', 'necesidad', 'de', 'tomar', 'medida', 'para', 'proteger', 'al', 'sistema', 'judicial', 'australiano', 'frente', 'a', 'uno', 'página', 'de', 'internet', 'que', 'imposibilitar', 'el', 'cumplimiento', 'de', 'el', 'principio', 'básico', 'de', 'el', 'Ley', '.', 'el', 'petición', 'del', 'Abogado', 'General', 'tener', 'lugar', 'después', 'de', 'que', 'uno', 'juez', 'del', 'Tribunal', 'Supremo', 'del', 'estado', 'de', 'Victoria', '(', 'Australia', ')', 'él', 'ver', 'forzado', 'a', 'disolver', 'uno', 'jurado', 'popular', 'y', 'suspender', 'el', 'proceso', 'ante', 'el', 'argumento', 'de', 'el', 'defensa', 'de', 'que', 'el', 'persona', 'que', 'él', 'compon', 'poder', 'haber', 'obtener', 'información', 'sobre', 'el', 'acusado', 'a', 'través', 'de', 'el', 'página', 'CrimeNet', '.', 'este', 'página', 'web', 'llevar', 'uno', 'mes', 'de', 'existencia', ',', 'tiempo', 'en', 'el', 'que', 'haber', 'ser', 'visitar', 'en', 'más', 'de', 'un', 'millón', 'de', 'ocasión', ',', 'y', 'facilitar', 'información', 'sobre', 'mil', 'de', 'crimen', 'y', 'criminal', 'ya', 'enjuiciado', 'o', 'aún', 'perseguido', ',', 'dato', 'que', 'salir', 'de', 'artículo', 'de', 'periódico', 'y', 'archivo', 'judicial', '.']\n"
     ]
    }
   ],
   "source": [
    "# Process a sample sentence using our data class\n",
    "sample_sentence = train[:5]\n",
    "sentence_processor = data(sample_sentence)\n",
    "\n",
    "# Get lemmas for the sentence\n",
    "lemmas = sentence_processor(language=\"es\")\n",
    "\n",
    "# Display the original words, POS tags, and BIO tags\n",
    "print(\"Words:\", sentence_processor.get_word())\n",
    "print(\"POS tags:\", sentence_processor.get_pos())\n",
    "print(\"BIO tags:\", sentence_processor.get_bio())\n",
    "print(\"Lemmas:\", lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1504bd36",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering for NER\n",
    "\n",
    "Let's implement an optimized feature function class for NER that includes:\n",
    "- Basic word features\n",
    "- Contextual features\n",
    "- POS tag features\n",
    "- Lemma features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8dbba93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedFeatureFunction:\n",
    "    def __init__(self, use_basic=True, use_context=True, use_pos=True, use_lemmas=False, use_specific=True):\n",
    "        self.use_basic = use_basic\n",
    "        self.use_context = use_context\n",
    "        self.use_pos = use_pos\n",
    "        self.use_lemmas = False  # Always set to False regardless of the input parameter\n",
    "        self.use_specific = use_specific\n",
    "        \n",
    "        # Location-related suffixes\n",
    "        self.loc_suffixes = {'ía', 'cia', 'dor', 'dal', 'guay', 'cha', 'nia', 'oz'}\n",
    "        \n",
    "        # Common organization precedents\n",
    "        self.org_precedents = {\n",
    "            \"presidente de la\",\n",
    "            \"el presidente del\",\n",
    "            \"portavoz del\",\n",
    "            \"general de la\",\n",
    "            \"director general de\",\n",
    "        }\n",
    "        \n",
    "        # Cache for performance\n",
    "        self.cache = {}\n",
    "        \n",
    "    def __call__(self, tokens, idx):\n",
    "        # Check if tokens contain strings or tuples\n",
    "        is_string_tokens = False\n",
    "        if tokens and isinstance(tokens[0], str):\n",
    "            is_string_tokens = True\n",
    "        \n",
    "        # Create a cache key for this token\n",
    "        if is_string_tokens:\n",
    "            sentence_key = tuple(tokens)\n",
    "        else:\n",
    "            # For tuples, create keys from the first element (word) only\n",
    "            sentence_key = tuple([(t[0]) for t in tokens])\n",
    "            \n",
    "        cache_key = (sentence_key, idx)\n",
    "        \n",
    "        # Check if we've already computed features for this token\n",
    "        if cache_key in self.cache:\n",
    "            return self.cache[cache_key]\n",
    "        \n",
    "        # Initialize feature dictionary\n",
    "        features = {}\n",
    "        \n",
    "        # Check bounds\n",
    "        if idx >= len(tokens) or idx < 0:\n",
    "            self.cache[cache_key] = features\n",
    "            return features\n",
    "        \n",
    "        # Handle different token formats (string vs tuple)\n",
    "        if is_string_tokens:\n",
    "            word = tokens[idx]\n",
    "            pos = None  # No POS tag available for string tokens\n",
    "            lemma = None  # No lemma available for string tokens\n",
    "        else:\n",
    "            # Current token info from tuple\n",
    "            word = tokens[idx][0]\n",
    "            pos = tokens[idx][1] if len(tokens[idx]) > 1 else None\n",
    "            lemma = tokens[idx][2] if len(tokens[idx]) > 2 else None\n",
    "        \n",
    "        # 1. Basic word features\n",
    "        if self.use_basic:\n",
    "            features[\"word\"] = word.lower()\n",
    "            features[\"length\"] = len(word)\n",
    "            \n",
    "            if word and word[0].isupper():\n",
    "                features[\"capitalized\"] = True\n",
    "                \n",
    "            if word.isupper() and len(word) > 1:\n",
    "                features[\"all_caps\"] = True\n",
    "                \n",
    "            if any(c.isdigit() for c in word):\n",
    "                features[\"has_digit\"] = True\n",
    "                \n",
    "            if any(c in string.punctuation for c in word):\n",
    "                features[\"has_punct\"] = True\n",
    "                \n",
    "            if len(word) > 1:\n",
    "                features[\"prefix\"] = word[:2]\n",
    "                features[\"suffix\"] = word[-2:]\n",
    "        \n",
    "        # 2. Context features - handle both string and tuple cases\n",
    "        if self.use_context:\n",
    "            if idx > 0:\n",
    "                if is_string_tokens:\n",
    "                    features[\"prev_word\"] = tokens[idx-1].lower()\n",
    "                else:\n",
    "                    features[\"prev_word\"] = tokens[idx-1][0].lower()\n",
    "                \n",
    "            if idx < len(tokens) - 1:\n",
    "                if is_string_tokens:\n",
    "                    features[\"next_word\"] = tokens[idx+1].lower()\n",
    "                else:\n",
    "                    features[\"next_word\"] = tokens[idx+1][0].lower()\n",
    "        \n",
    "        # 3. POS tag features - only if we have POS info\n",
    "        if self.use_pos and pos is not None:\n",
    "            features[\"pos\"] = pos\n",
    "            \n",
    "            if idx > 0 and not is_string_tokens:\n",
    "                features[\"prev_pos\"] = tokens[idx-1][1]\n",
    "                \n",
    "            if idx < len(tokens) - 1 and not is_string_tokens:\n",
    "                features[\"next_pos\"] = tokens[idx+1][1]\n",
    "        \n",
    "        # 4. Lemma features - always skipped since we set self.use_lemmas = False\n",
    "        # The code below will never execute\n",
    "        if self.use_lemmas and lemma is not None:\n",
    "            features[\"lemma\"] = lemma\n",
    "        \n",
    "        # 5. Specific NER features\n",
    "        if self.use_specific:\n",
    "            # Check for location suffixes\n",
    "            if len(word) > 2:\n",
    "                for suffix in self.loc_suffixes:\n",
    "                    if word.lower().endswith(suffix):\n",
    "                        features[\"loc_suffix\"] = True\n",
    "                        break\n",
    "            \n",
    "            # Check for organization precedents - adapt for both token types\n",
    "            if idx >= 3:\n",
    "                if is_string_tokens:\n",
    "                    word1 = tokens[idx-3]\n",
    "                    word2 = tokens[idx-2]\n",
    "                    word3 = tokens[idx-1]\n",
    "                else:\n",
    "                    word1 = tokens[idx-3][0]\n",
    "                    word2 = tokens[idx-2][0]\n",
    "                    word3 = tokens[idx-1][0]\n",
    "\n",
    "                trigram = f\"{word1} {word2} {word3}\".lower()\n",
    "                if any(precedent in trigram for precedent in self.org_precedents):\n",
    "                    features[\"org_precedent\"] = True\n",
    "        \n",
    "        # Cache and return features\n",
    "        self.cache[cache_key] = features\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4eb782d",
   "metadata": {},
   "source": [
    "## 4. Prepare Data for CRF Model\n",
    "\n",
    "Now let's prepare our data for the CRF model, including lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e34d3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "[('Melbourne', 'NP', 'B-LOC'), ('(', 'Fpa', 'O'), ('Australia', 'NP', 'B-LOC'), (')', 'Fpt', 'O'), (',', 'Fc', 'O'), ('25', 'Z', 'O'), ('may', 'NC', 'O'), ('(', 'Fpa', 'O'), ('EFE', 'NC', 'B-ORG'), (')', 'Fpt', 'O'), ('.', 'Fp', 'O')]\n",
      "\n",
      "Processed (with lemmas):\n",
      "[(('Melbourne', 'NP', 'Melbourne'), 'B-LOC'), (('(', 'Fpa', '('), 'O'), (('Australia', 'NP', 'Australia'), 'B-LOC'), ((')', 'Fpt', ')'), 'O'), ((',', 'Fc', ','), 'O'), (('25', 'Z', '25'), 'O'), (('may', 'NC', 'may'), 'O'), (('(', 'Fpa', '('), 'O'), (('EFE', 'NC', 'EFE'), 'B-ORG'), ((')', 'Fpt', ')'), 'O'), (('.', 'Fp', '.'), 'O')]\n"
     ]
    }
   ],
   "source": [
    "# Load SpaCy model for Spanish\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "def prepare_data_for_crf(conll_data, include_lemmas=True):\n",
    "    \"\"\"Process conll data into format for CRF tagging with optional lemmatization\"\"\"\n",
    "    processed_data = []\n",
    "    \n",
    "    for sentence in conll_data:\n",
    "        # Process entire sentence for better lemmatization context\n",
    "        if include_lemmas:\n",
    "            text = \" \".join(word for word, _, _ in sentence)\n",
    "            doc = nlp(text)\n",
    "            \n",
    "            # Create processed sentence\n",
    "            processed_sentence = []\n",
    "            for i, (word, pos, tag) in enumerate(sentence):\n",
    "                if i < len(doc):\n",
    "                    lemma = doc[i].lemma_\n",
    "                    processed_sentence.append(((word, pos, lemma), tag))\n",
    "                else:\n",
    "                    # Fallback in case of token mismatch\n",
    "                    processed_sentence.append(((word, pos, word.lower()), tag))\n",
    "        else:\n",
    "            processed_sentence = [((word, pos), tag) for word, pos, tag in sentence]\n",
    "            \n",
    "        processed_data.append(processed_sentence)\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "# For demonstration, process just a small sample\n",
    "sample_data = train\n",
    "processed_sample = prepare_data_for_crf(sample_data, include_lemmas=True)\n",
    "\n",
    "# Show the first processed sentence\n",
    "print(\"Original:\")\n",
    "print(sample_data[0])\n",
    "print(\"\\nProcessed (with lemmas):\")\n",
    "print(processed_sample[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb323ce9",
   "metadata": {},
   "source": [
    "## 5. Train a CRF Model for NER\n",
    "\n",
    "Let's train a small CRF model using our feature function and prepared data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "abde6cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRF model trained!\n"
     ]
    }
   ],
   "source": [
    "# Create our feature function\n",
    "feature_func = OptimizedFeatureFunction(\n",
    "    use_basic=True,\n",
    "    use_context=True, \n",
    "    use_pos=True,\n",
    "    use_lemmas=False,  # Set to False explicitly (though the class will ignore this anyway)\n",
    "    use_specific=True\n",
    ")\n",
    "\n",
    "# Initialize CRF tagger with our feature function\n",
    "crf_tagger = CRFTagger(feature_func=feature_func)\n",
    "\n",
    "# For demonstration, train on a small subset\n",
    "small_train = processed_sample\n",
    "crf_tagger.train(small_train, 'example_model.crf.tagger')\n",
    "\n",
    "print(\"CRF model trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2517cd",
   "metadata": {},
   "source": [
    "## 6. Experiment with Different Tag Encoding Schemes\n",
    "\n",
    "The BIO scheme is the most common, but let's implement functions to convert to other schemes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "789fa5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original (BIO):\n",
      "[('Por', 'SP', 'O'), ('su', 'DP', 'O'), ('parte', 'NC', 'O'), (',', 'Fc', 'O'), ('el', 'DA', 'O'), ('Abogado', 'NC', 'B-PER'), ('General', 'AQ', 'I-PER'), ('de', 'SP', 'O'), ('Victoria', 'NC', 'B-LOC'), (',', 'Fc', 'O'), ('Rob', 'NC', 'B-PER'), ('Hulls', 'AQ', 'I-PER'), (',', 'Fc', 'O'), ('indicó', 'VMI', 'O'), ('que', 'CS', 'O'), ('no', 'RN', 'O'), ('hay', 'VAI', 'O'), ('nadie', 'PI', 'O'), ('que', 'PR', 'O'), ('controle', 'VMS', 'O'), ('que', 'CS', 'O'), ('las', 'DA', 'O'), ('informaciones', 'NC', 'O'), ('contenidas', 'AQ', 'O'), ('en', 'SP', 'O'), ('CrimeNet', 'NC', 'B-MISC'), ('son', 'VSI', 'O'), ('veraces', 'AQ', 'O'), ('.', 'Fp', 'O')]\n",
      "\n",
      "IO Scheme:\n",
      "[('Por', 'SP', 'O'), ('su', 'DP', 'O'), ('parte', 'NC', 'O'), (',', 'Fc', 'O'), ('el', 'DA', 'O'), ('Abogado', 'NC', 'I-PER'), ('General', 'AQ', 'I-PER'), ('de', 'SP', 'O'), ('Victoria', 'NC', 'I-LOC'), (',', 'Fc', 'O'), ('Rob', 'NC', 'I-PER'), ('Hulls', 'AQ', 'I-PER'), (',', 'Fc', 'O'), ('indicó', 'VMI', 'O'), ('que', 'CS', 'O'), ('no', 'RN', 'O'), ('hay', 'VAI', 'O'), ('nadie', 'PI', 'O'), ('que', 'PR', 'O'), ('controle', 'VMS', 'O'), ('que', 'CS', 'O'), ('las', 'DA', 'O'), ('informaciones', 'NC', 'O'), ('contenidas', 'AQ', 'O'), ('en', 'SP', 'O'), ('CrimeNet', 'NC', 'I-MISC'), ('son', 'VSI', 'O'), ('veraces', 'AQ', 'O'), ('.', 'Fp', 'O')]\n",
      "\n",
      "BIOES Scheme:\n",
      "[('Por', 'SP', 'O'), ('su', 'DP', 'O'), ('parte', 'NC', 'O'), (',', 'Fc', 'O'), ('el', 'DA', 'O'), ('Abogado', 'NC', 'B-PER'), ('General', 'AQ', 'E-PER'), ('de', 'SP', 'O'), ('Victoria', 'NC', 'S-LOC'), (',', 'Fc', 'O'), ('Rob', 'NC', 'B-PER'), ('Hulls', 'AQ', 'E-PER'), (',', 'Fc', 'O'), ('indicó', 'VMI', 'O'), ('que', 'CS', 'O'), ('no', 'RN', 'O'), ('hay', 'VAI', 'O'), ('nadie', 'PI', 'O'), ('que', 'PR', 'O'), ('controle', 'VMS', 'O'), ('que', 'CS', 'O'), ('las', 'DA', 'O'), ('informaciones', 'NC', 'O'), ('contenidas', 'AQ', 'O'), ('en', 'SP', 'O'), ('CrimeNet', 'NC', 'S-MISC'), ('son', 'VSI', 'O'), ('veraces', 'AQ', 'O'), ('.', 'Fp', 'O')]\n"
     ]
    }
   ],
   "source": [
    "def bio_to_io(tagged_sent):\n",
    "    \"\"\"Convert BIO tagging to IO tagging\"\"\"\n",
    "    io_sent = []\n",
    "    for word, pos, tag in tagged_sent:\n",
    "        if tag == \"O\":\n",
    "            io_sent.append((word, pos, tag))\n",
    "        else:\n",
    "            # Replace B- with I- for any entity tag\n",
    "            entity_type = tag[2:]\n",
    "            io_sent.append((word, pos, f\"I-{entity_type}\"))\n",
    "    return io_sent\n",
    "\n",
    "def bio_to_bioes(sent):\n",
    "    \"\"\"Convert BIO tagging to BIOES tagging\"\"\"\n",
    "    new_sent = []\n",
    "    n = len(sent)\n",
    "    i = 0\n",
    "    \n",
    "    while i < n:\n",
    "        word, pos, tag = sent[i]\n",
    "        \n",
    "        if tag == \"O\":\n",
    "            new_sent.append((word, pos, tag))\n",
    "            i += 1\n",
    "        elif tag.startswith(\"B-\"):\n",
    "            entity_type = tag[2:]\n",
    "            \n",
    "            # Check if it's a singleton entity (no following I- tags)\n",
    "            if i + 1 == n or not sent[i+1][2].startswith(f\"I-{entity_type}\"):\n",
    "                new_sent.append((word, pos, f\"S-{entity_type}\"))\n",
    "                i += 1\n",
    "            else:\n",
    "                # It's the beginning of a multi-token entity\n",
    "                new_sent.append((word, pos, f\"B-{entity_type}\"))\n",
    "                i += 1\n",
    "                \n",
    "                # Process all the intermediate I- tags\n",
    "                while i < n and sent[i][2] == f\"I-{entity_type}\":\n",
    "                    # Check if this is the last I- tag\n",
    "                    if i + 1 == n or sent[i+1][2] != f\"I-{entity_type}\":\n",
    "                        new_sent.append((sent[i][0], sent[i][1], f\"E-{entity_type}\"))\n",
    "                    else:\n",
    "                        new_sent.append((sent[i][0], sent[i][1], f\"I-{entity_type}\"))\n",
    "                    i += 1\n",
    "        else:\n",
    "            # Handle unexpected tags (like I- without preceding B-)\n",
    "            new_sent.append((word, pos, tag))\n",
    "            i += 1\n",
    "            \n",
    "    return new_sent\n",
    "\n",
    "# Example of converting tags\n",
    "sample_sent = train[5]  # Get a sentence that hopefully has some entities\n",
    "print(\"Original (BIO):\")\n",
    "print(sample_sent)\n",
    "\n",
    "io_sent = bio_to_io(sample_sent)\n",
    "print(\"\\nIO Scheme:\")\n",
    "print(io_sent)\n",
    "\n",
    "bioes_sent = bio_to_bioes(sample_sent)\n",
    "print(\"\\nBIOES Scheme:\")\n",
    "print(bioes_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9e0e53",
   "metadata": {},
   "source": [
    "## 7. Entity-Level Evaluation\n",
    "\n",
    "Instead of just token-level accuracy, let's implement entity-level evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9030cda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(tags):\n",
    "    \"\"\"\n",
    "    Extract entity spans from a sequence of BIO tags.\n",
    "    \n",
    "    Args:\n",
    "        tags: List of BIO tags (e.g., 'B-PER', 'I-PER', 'O')\n",
    "        \n",
    "    Returns:\n",
    "        List of tuples (entity_type, start_idx, end_idx)\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    entity_type = None\n",
    "    start_idx = None\n",
    "    \n",
    "    for i, tag in enumerate(tags):\n",
    "        # Handle the case where tag might be a tuple\n",
    "        if isinstance(tag, tuple):\n",
    "            tag = tag[1]  # Extract the actual tag if it's a tuple (word, tag)\n",
    "            \n",
    "        if tag.startswith('B-'):\n",
    "            # If we were tracking an entity, add it to the list\n",
    "            if entity_type is not None:\n",
    "                entities.append((entity_type, start_idx, i - 1))\n",
    "            # Start a new entity\n",
    "            entity_type = tag[2:]  # Remove 'B-' prefix\n",
    "            start_idx = i\n",
    "        elif tag.startswith('I-'):\n",
    "            # Continue with the current entity\n",
    "            curr_type = tag[2:]  # Remove 'I-' prefix\n",
    "            # This handles inconsistent I- tags that don't match the current entity\n",
    "            if entity_type is None or curr_type != entity_type:\n",
    "                # Close any open entity and ignore this tag (it's an error in tagging)\n",
    "                if entity_type is not None:\n",
    "                    entities.append((entity_type, start_idx, i - 1))\n",
    "                entity_type = None\n",
    "                start_idx = None\n",
    "        else:  # 'O' tag\n",
    "            # If we were tracking an entity, add it to the list\n",
    "            if entity_type is not None:\n",
    "                entities.append((entity_type, start_idx, i - 1))\n",
    "                entity_type = None\n",
    "                start_idx = None\n",
    "    \n",
    "    # Don't forget the last entity if the sequence ends with an entity\n",
    "    if entity_type is not None:\n",
    "        entities.append((entity_type, start_idx, len(tags) - 1))\n",
    "    \n",
    "    return entities\n",
    "\n",
    "\n",
    "def evaluate_entities(gold_entities, pred_entities):\n",
    "    \"\"\"\n",
    "    Calculate precision, recall, and F1 score for entity recognition.\n",
    "    \n",
    "    Args:\n",
    "        gold_entities: List of gold standard entity tuples (type, start, end)\n",
    "        pred_entities: List of predicted entity tuples (type, start, end)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with precision, recall, and F1 scores\n",
    "    \"\"\"\n",
    "    # Convert to sets for easier comparison\n",
    "    gold_set = set(gold_entities)\n",
    "    pred_set = set(pred_entities)\n",
    "    \n",
    "    # Calculate correct predictions (intersection)\n",
    "    correct = len(gold_set.intersection(pred_set))\n",
    "    \n",
    "    # Calculate precision, recall, and F1\n",
    "    precision = correct / len(pred_set) if pred_set else 0.0\n",
    "    recall = correct / len(gold_set) if gold_set else 1.0  # Perfect recall if no gold entities\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'gold_count': len(gold_set),\n",
    "        'pred_count': len(pred_set),\n",
    "        'correct': correct\n",
    "    }\n",
    "\n",
    "def evaluate_ner_corpus(gold_data, predicted_data):\n",
    "    \"\"\"\n",
    "    Evaluate NER performance at entity level across an entire corpus.\n",
    "    \n",
    "    Args:\n",
    "        gold_data: List of sentences where each sentence is a list of (word, gold_tag) tuples\n",
    "        predicted_data: List of sentences where each sentence is a list of (word, pred_tag) tuples\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with overall precision, recall, and F1 scores\n",
    "    \"\"\"\n",
    "    total_correct = 0\n",
    "    total_gold = 0\n",
    "    total_pred = 0\n",
    "    \n",
    "    for gold_sent, pred_sent in zip(gold_data, predicted_data):\n",
    "        # Extract just the tags\n",
    "        gold_tags = [tag for _, tag in gold_sent]\n",
    "        pred_tags = [tag for _, tag in pred_sent]\n",
    "        \n",
    "        # Extract entities\n",
    "        gold_entities = extract_entities(gold_tags)\n",
    "        pred_entities = extract_entities(pred_tags)\n",
    "        \n",
    "        # Evaluate this sentence\n",
    "        results = evaluate_entities(gold_entities, pred_entities)\n",
    "        \n",
    "        # Accumulate counts\n",
    "        total_correct += results['correct']\n",
    "        total_gold += results['gold_count']\n",
    "        total_pred += results['pred_count']\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    precision = total_correct / total_pred if total_pred > 0 else 0.0\n",
    "    recall = total_correct / total_gold if total_gold > 0 else 1.0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    accuracy = total_correct / total_gold\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'accuracy': accuracy  # Using F1 as the \"accuracy\" metric for entity-level evaluation\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec51548e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity-Level Evaluation:\n",
      "Precision: 0.6171\n",
      "Recall: 0.5450\n",
      "F1 Score: 0.5788\n",
      "\n",
      "Token-Level Accuracy: 0.9115\n",
      "\n",
      "Token-Level Accuracy: 0.9115\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Extend CRFTagger to support entity-level evaluation\n",
    "def entity_level_accuracy(tagger, test_data):\n",
    "    \"\"\"\n",
    "    Calculate entity-level evaluation metrics for a CRFTagger.\n",
    "    \n",
    "    Args:\n",
    "        tagger: Trained CRFTagger model\n",
    "        test_data: List of sentences where each sentence is a list of (word, pos, tag) tuples\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with precision, recall, F1, and accuracy scores\n",
    "    \"\"\"\n",
    "    # Convert test data to the format expected by the evaluation function\n",
    "    formatted_test_data = [[(word, label) for (word, pos, label) in sent] for sent in test_data]\n",
    "    \n",
    "    # Get predictions\n",
    "    predicted_data = []\n",
    "    for sentence in test_data:  # Use original test_data to extract words\n",
    "        words = [word for word, _, _ in sentence]\n",
    "        tags = tagger.tag(words)\n",
    "        predicted_data.append(list(zip(words, tags)))\n",
    "    \n",
    "    # Evaluate\n",
    "    results = evaluate_ner_corpus(formatted_test_data, predicted_data)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Replace the token-level accuracy evaluation with entity-level evaluation\n",
    "entity_metrics = entity_level_accuracy(crf_tagger, test)  # Changed from trained_tagger to crf_tagger\n",
    "\n",
    "print(\"Entity-Level Evaluation:\")\n",
    "print(f\"Precision: {entity_metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {entity_metrics['recall']:.4f}\")\n",
    "print(f\"F1 Score: {entity_metrics['f1']:.4f}\")\n",
    "\n",
    "# For comparison, also show the token-level accuracy\n",
    "# First prepare test data in the format expected by the CRF tagger\n",
    "test_data = [[(word, label) for (word, pos, label) in sent] for sent in test]\n",
    "token_accuracy = crf_tagger.accuracy(test_data)  # Changed from trained_tagger to crf_tagger\n",
    "print(f\"\\nToken-Level Accuracy: {token_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8872659",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "This notebook has demonstrated:\n",
    "\n",
    "1. How to use the `data` class for processing NER data\n",
    "2. How to implement and customize a feature function\n",
    "3. How to prepare data for CRF tagging\n",
    "4. How to convert between different tagging schemes (BIO, IO, BIOES)\n",
    "5. How to perform entity-level evaluation\n",
    "\n",
    "These techniques can be applied to improve NER performance for Spanish and other languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02054101",
   "metadata": {},
   "source": [
    "## 9. Feature Combination Analysis\n",
    "\n",
    "Let's evaluate different combinations of features to find the optimal configuration using entity-level evaluation metrics instead of token-level accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c88fc81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing configuration 1/31: {'Basic': True, 'Context_Words': False, 'Context_POS': False, 'Specific': False, 'Lemmas': False}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 194\u001b[0m\n\u001b[0;32m    191\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m    193\u001b[0m \u001b[38;5;66;03m# Uncomment to run the evaluation (it may take a while)\u001b[39;00m\n\u001b[1;32m--> 194\u001b[0m results_df \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_feature_combinations\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[30], line 34\u001b[0m, in \u001b[0;36mevaluate_feature_combinations\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Create and train model with this feature configuration\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m tagger \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_crf_with_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Using a subset for demonstration\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Calculate entity-level metrics\u001b[39;00m\n\u001b[0;32m     37\u001b[0m entity_metrics \u001b[38;5;241m=\u001b[39m entity_level_accuracy(tagger, test)\n",
      "Cell \u001b[1;32mIn[30], line 92\u001b[0m, in \u001b[0;36mtrain_crf_with_config\u001b[1;34m(training_data, config)\u001b[0m\n\u001b[0;32m     90\u001b[0m pos_tags \u001b[38;5;241m=\u001b[39m sentence_processor\u001b[38;5;241m.\u001b[39mget_pos()\n\u001b[0;32m     91\u001b[0m bio_tags \u001b[38;5;241m=\u001b[39m sentence_processor\u001b[38;5;241m.\u001b[39mget_bio()\n\u001b[1;32m---> 92\u001b[0m lemmas \u001b[38;5;241m=\u001b[39m \u001b[43msentence_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# Format the data for CRF training\u001b[39;00m\n\u001b[0;32m     95\u001b[0m processed_sentence \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\AI4\\PLH\\Language-detection\\Classificación_secuencias\\Pruebas_Hui.py:61\u001b[0m, in \u001b[0;36mdata.__call__\u001b[1;34m(self, language)\u001b[0m\n\u001b[0;32m     59\u001b[0m     lemmas \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_words:\n\u001b[1;32m---> 61\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m         lemmas\u001b[38;5;241m.\u001b[39mappend(doc[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mlemma_)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lemmas\n",
      "File \u001b[1;32mc:\\Users\\jiahu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\language.py:1052\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1050\u001b[0m     error_handler \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1052\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcomponent_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1054\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jiahu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:52\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\jiahu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\pipeline\\tok2vec.py:126\u001b[0m, in \u001b[0;36mTok2Vec.predict\u001b[1;34m(self, docs)\u001b[0m\n\u001b[0;32m    124\u001b[0m     width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnO\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39malloc((\u001b[38;5;241m0\u001b[39m, width)) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n\u001b[1;32m--> 126\u001b[0m tokvecs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokvecs\n",
      "File \u001b[1;32mc:\\Users\\jiahu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\thinc\\model.py:334\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OutT:\n\u001b[0;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;124;03m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\jiahu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[1;32mc:\\Users\\jiahu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jiahu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\thinc\\layers\\with_array.py:42\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, Xseq, is_train)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m](Xseq, is_train)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], \u001b[43m_list_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\jiahu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\thinc\\layers\\with_array.py:77\u001b[0m, in \u001b[0;36m_list_forward\u001b[1;34m(model, Xs, is_train)\u001b[0m\n\u001b[0;32m     75\u001b[0m lengths \u001b[38;5;241m=\u001b[39m NUMPY_OPS\u001b[38;5;241m.\u001b[39masarray1i([\u001b[38;5;28mlen\u001b[39m(seq) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m Xs])\n\u001b[0;32m     76\u001b[0m Xf \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mflatten(Xs, pad\u001b[38;5;241m=\u001b[39mpad)\n\u001b[1;32m---> 77\u001b[0m Yf, get_dXf \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackprop\u001b[39m(dYs: ListXd) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ListXd:\n\u001b[0;32m     80\u001b[0m     dYf \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mflatten(dYs, pad\u001b[38;5;241m=\u001b[39mpad)\n",
      "File \u001b[1;32mc:\\Users\\jiahu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jiahu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[1;32mc:\\Users\\jiahu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jiahu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\thinc\\layers\\residual.py:41\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m d_output \u001b[38;5;241m+\u001b[39m dX\n\u001b[1;32m---> 41\u001b[0m Y, backprop_layer \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [X[i] \u001b[38;5;241m+\u001b[39m Y[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X))], backprop\n",
      "File \u001b[1;32mc:\\Users\\jiahu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jiahu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[1;32mc:\\Users\\jiahu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jiahu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "    \u001b[1;31m[... skipping similar frames: Model.__call__ at line 310 (1 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\jiahu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[1;32mc:\\Users\\jiahu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jiahu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\thinc\\layers\\maxout.py:52\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     50\u001b[0m W \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_param(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     51\u001b[0m W \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mreshape2f(W, nO \u001b[38;5;241m*\u001b[39m nP, nI)\n\u001b[1;32m---> 52\u001b[0m Y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgemm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrans2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m Y \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mreshape1f(b, nO \u001b[38;5;241m*\u001b[39m nP)\n\u001b[0;32m     54\u001b[0m Z \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mreshape3f(Y, Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], nO, nP)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "import time\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_feature_combinations():\n",
    "    # Define feature groups to test\n",
    "    feature_groups = {\n",
    "        \"Basic\": True,        # word, length, etc.\n",
    "        \"Context_Words\": True,  # prev_word, next_word\n",
    "        \"Context_POS\": True,    # POS tags of surrounding words\n",
    "        \"Specific\": True,      # location_suffix, organization_precedent, etc.\n",
    "        \"Lemmas\": True,        # Use lemmatization features\n",
    "    }\n",
    "    \n",
    "    # Generate all possible combinations of feature groups\n",
    "    all_configs = []\n",
    "    feature_names = list(feature_groups.keys())\n",
    "    \n",
    "    for r in range(1, len(feature_names) + 1):\n",
    "        for combo in combinations(feature_names, r):\n",
    "            config = {name: (name in combo) for name in feature_names}\n",
    "            all_configs.append(config)\n",
    "    \n",
    "    # Create a dataframe to store results\n",
    "    results = []\n",
    "    \n",
    "    # Evaluate each configuration\n",
    "    for i, config in enumerate(all_configs):\n",
    "        print(f\"Testing configuration {i+1}/{len(all_configs)}: {config}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create and train model with this feature configuration\n",
    "        tagger = train_crf_with_config(train, config)  # Using a subset for demonstration\n",
    "        \n",
    "        # Calculate entity-level metrics\n",
    "        entity_metrics = entity_level_accuracy(tagger, test)\n",
    "        \n",
    "        # Calculate elapsed time\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'Configuration': str(config),\n",
    "            'Basic': config['Basic'],\n",
    "            'Context_Words': config['Context_Words'],\n",
    "            'Context_POS': config['Context_POS'],\n",
    "            'Specific': config['Specific'],\n",
    "            'Lemmas': config['Lemmas'],\n",
    "            'Precision': entity_metrics['precision'],\n",
    "            'Recall': entity_metrics['recall'],\n",
    "            'F1': entity_metrics['f1'],\n",
    "            'Time_Seconds': elapsed\n",
    "        })\n",
    "        \n",
    "        print(f\"  F1 Score: {entity_metrics['f1']:.4f}, Time: {elapsed:.2f} seconds\")\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Sort by F1 score\n",
    "    results_df = results_df.sort_values('F1', ascending=False)\n",
    "    \n",
    "    # Display best configurations\n",
    "    print(\"\\n--- Top 5 Feature Configurations ---\")\n",
    "    print(results_df.head(5))\n",
    "    \n",
    "    # Create plots to visualize results\n",
    "    plot_feature_importance(results_df)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def train_crf_with_config(training_data, config):\n",
    "    # Create feature function with the specified configuration\n",
    "    feat_func = OptimizedFeatureFunction(\n",
    "        use_basic=config['Basic'], \n",
    "        use_context=config['Context_Words'], \n",
    "        use_pos=config['Context_POS'], \n",
    "        use_specific=config['Specific'],\n",
    "        use_lemmas=config['Lemmas']\n",
    "    )\n",
    "    \n",
    "    # Process data for CRF training\n",
    "    processed_data = []\n",
    "    \n",
    "    \n",
    "    # Process with our data class to get lemmas\n",
    "    sentence_processor = data(training_data)\n",
    "    words = sentence_processor.get_word()\n",
    "    pos_tags = sentence_processor.get_pos()\n",
    "    bio_tags = sentence_processor.get_bio()\n",
    "    lemmas = sentence_processor(language=\"es\")\n",
    "    \n",
    "    # Format the data for CRF training\n",
    "    processed_sentence = []\n",
    "    for i, (word, pos, bio) in enumerate(zip(words, pos_tags, bio_tags)):\n",
    "        if i < len(lemmas):  # Ensure we have a lemma for this word\n",
    "            processed_sentence.append(((word, pos, lemmas[i]), bio))\n",
    "        else:\n",
    "            # Fallback in case of token mismatch\n",
    "            processed_sentence.append(((word, pos, word.lower()), bio))\n",
    "            \n",
    "    processed_data.append(processed_sentence)\n",
    "    \n",
    "    # Create and train the CRF tagger\n",
    "    ct = CRFTagger(feature_func=feat_func)\n",
    "    ct.train(processed_data, 'temp_model.crf.tagger')  # Temporary file for evaluation\n",
    "    \n",
    "    return ct\n",
    "\n",
    "def plot_feature_importance(results_df):\n",
    "    # Create figure with multiple subplots\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Plot 1: F1 score distribution\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.hist(results_df['F1'], bins=15, color='skyblue', edgecolor='black')\n",
    "    plt.xlabel('F1 Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of F1 Scores')\n",
    "    \n",
    "    # Plot 2: Effect of each feature on F1 score\n",
    "    plt.subplot(2, 2, 2)\n",
    "    feature_cols = ['Basic', 'Context_Words', 'Context_POS', 'Specific', 'Lemmas']\n",
    "    \n",
    "    # Calculate mean F1 for each feature when it's present vs absent\n",
    "    feature_effects = {}\n",
    "    for feat in feature_cols:\n",
    "        present_mean = results_df[results_df[feat] == True]['F1'].mean()\n",
    "        absent_mean = results_df[results_df[feat] == False]['F1'].mean()\n",
    "        feature_effects[feat] = present_mean - absent_mean\n",
    "    \n",
    "    # Plot the effects\n",
    "    plt.bar(feature_effects.keys(), feature_effects.values(), color='lightgreen')\n",
    "    plt.axhline(y=0, color='r', linestyle='-', alpha=0.3)\n",
    "    plt.ylabel('Mean F1 Score Difference')\n",
    "    plt.title('Effect of Each Feature on F1 Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Plot 3: F1 vs number of features enabled\n",
    "    plt.subplot(2, 2, 3)\n",
    "    results_df['Features_Enabled'] = results_df[feature_cols].sum(axis=1)\n",
    "    \n",
    "    # Group by number of features and calculate mean F1\n",
    "    grouped = results_df.groupby('Features_Enabled')['F1'].mean().reset_index()\n",
    "    \n",
    "    plt.plot(grouped['Features_Enabled'], grouped['F1'], marker='o', linestyle='-')\n",
    "    plt.xlabel('Number of Features Enabled')\n",
    "    plt.ylabel('Mean F1 Score')\n",
    "    plt.title('F1 Score vs Feature Count')\n",
    "    \n",
    "    # Plot 4: Training time comparison\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.scatter(results_df['Time_Seconds'], results_df['F1'], alpha=0.6)\n",
    "    plt.xlabel('Time (seconds)')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('F1 Score vs Training Time')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Additional plot: feature combinations heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Create dummy binary columns for each feature combination\n",
    "    for i, feat1 in enumerate(feature_cols):\n",
    "        for j, feat2 in enumerate(feature_cols[i+1:], i+1):\n",
    "            results_df[f\"{feat1}_{feat2}\"] = results_df[feat1] & results_df[feat2]\n",
    "    \n",
    "    # Calculate mean F1 for each feature combination\n",
    "    combo_effects = {}\n",
    "    for combo in [f\"{feat1}_{feat2}\" for i, feat1 in enumerate(feature_cols) \n",
    "                  for j, feat2 in enumerate(feature_cols[i+1:], i+1)]:\n",
    "        if combo in results_df.columns:\n",
    "            combo_effects[combo] = results_df[results_df[combo]]['F1'].mean()\n",
    "    \n",
    "    # Create heatmap data\n",
    "    heatmap_data = pd.DataFrame(index=feature_cols, columns=feature_cols, data=0.0)\n",
    "    for i, feat1 in enumerate(feature_cols):\n",
    "        heatmap_data.loc[feat1, feat1] = results_df[results_df[feat1]]['F1'].mean()\n",
    "        for feat2 in feature_cols[i+1:]:\n",
    "            combo = f\"{feat1}_{feat2}\"\n",
    "            if combo in combo_effects:\n",
    "                heatmap_data.loc[feat1, feat2] = combo_effects[combo]\n",
    "                heatmap_data.loc[feat2, feat1] = combo_effects[combo]\n",
    "    \n",
    "    # Plot heatmap\n",
    "    sns.heatmap(heatmap_data, annot=True, cmap=\"YlGnBu\", fmt=\".4f\")\n",
    "    plt.title('Feature Combination Effectiveness (Mean F1 Score)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Uncomment to run the evaluation (it may take a while)\n",
    "results_df = evaluate_feature_combinations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69d3e4d",
   "metadata": {},
   "source": [
    "## 10. Full Analysis with Optimized Features\n",
    "\n",
    "Now that we've identified the best feature combinations, we can train a model with the optimal configuration and evaluate it on the full test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f24d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of running a complete analysis with the optimal feature configuration\n",
    "def run_optimal_configuration():\n",
    "    # Create feature function with optimal settings (example - replace with your findings)\n",
    "    optimal_feat_func = OptimizedFeatureFunction(\n",
    "        use_basic=True,\n",
    "        use_context=True, \n",
    "        use_pos=True,\n",
    "        use_lemmas=True,  # Including lemmatization\n",
    "        use_specific=True\n",
    "    )\n",
    "    \n",
    "    print(\"Processing training data...\")\n",
    "    # Process the training data\n",
    "    processed_train = []\n",
    "    for i, sentence in enumerate(train):\n",
    "        if i % 500 == 0:  # Progress indicator\n",
    "            print(f\"Processing sentence {i}/{len(train)}...\")\n",
    "        \n",
    "        # Use our data class to process the sentence\n",
    "        sentence_processor = data(sentence)\n",
    "        words = sentence_processor.get_word()\n",
    "        pos_tags = sentence_processor.get_pos()\n",
    "        bio_tags = sentence_processor.get_bio()\n",
    "        lemmas = sentence_processor(language=\"es\")  # Get lemmas\n",
    "        \n",
    "        # Format for CRF training\n",
    "        processed_sentence = []\n",
    "        for i, (word, pos, bio) in enumerate(zip(words, pos_tags, bio_tags)):\n",
    "            if i < len(lemmas):\n",
    "                processed_sentence.append(((word, pos, lemmas[i]), bio))\n",
    "            else:\n",
    "                processed_sentence.append(((word, pos, word.lower()), bio))\n",
    "                \n",
    "        processed_train.append(processed_sentence)\n",
    "    \n",
    "    print(\"Training CRF model...\")\n",
    "    # Train the model\n",
    "    optimal_tagger = CRFTagger(feature_func=optimal_feat_func)\n",
    "    optimal_tagger.train(processed_train, 'optimal_model.crf.tagger')\n",
    "    \n",
    "    print(\"Processing test data...\")\n",
    "    # Process the test data\n",
    "    processed_test = []\n",
    "    for sentence in test:\n",
    "        # Use our data class to process the sentence\n",
    "        sentence_processor = data(sentence)\n",
    "        words = sentence_processor.get_word()\n",
    "        pos_tags = sentence_processor.get_pos()\n",
    "        bio_tags = sentence_processor.get_bio()\n",
    "        lemmas = sentence_processor(language=\"es\")  # Get lemmas\n",
    "        \n",
    "        # Format for evaluation\n",
    "        processed_sentence = []\n",
    "        for i, (word, pos, bio) in enumerate(zip(words, pos_tags, bio_tags)):\n",
    "            if i < len(lemmas):\n",
    "                processed_sentence.append(((word, pos, lemmas[i]), bio))\n",
    "            else:\n",
    "                processed_sentence.append(((word, pos, word.lower()), bio))\n",
    "                \n",
    "        processed_test.append(processed_sentence)\n",
    "    \n",
    "    # Evaluate using entity-level metrics\n",
    "    entity_results = entity_level_accuracy(optimal_tagger, test)\n",
    "    \n",
    "    print(\"\\n=== Entity-Level Evaluation Results ===\")\n",
    "    print(f\"Precision: {entity_results['precision']:.4f}\")\n",
    "    print(f\"Recall: {entity_results['recall']:.4f}\")\n",
    "    print(f\"F1 Score: {entity_results['f1']:.4f}\")\n",
    "    \n",
    "    return optimal_tagger, entity_results\n",
    "\n",
    "# Uncomment to run the complete analysis (it will take a significant amount of time)\n",
    "# optimal_tagger, results = run_optimal_configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49987bbc",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated:\n",
    "\n",
    "1. How to use the custom `data` class to process NER data and extract lemmas\n",
    "2. Implementing a feature function class with various customizable feature sets\n",
    "3. Training and evaluating CRF models for NER with different feature combinations\n",
    "4. Converting between different tagging schemes (BIO, IO, BIOES)\n",
    "5. Entity-level evaluation for more accurate performance measurement\n",
    "6. Systematic analysis of feature importance for NER performance\n",
    "\n",
    "The entity-level evaluation provides a more meaningful assessment of NER performance than token-level accuracy, as it ensures that complete entities are correctly identified rather than just individual tokens."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
