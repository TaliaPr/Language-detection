{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import movie_reviews as mr\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mirar como es el fichero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(mr.fileids()) )#hay 2000 ficheros (1000 exemples positius i 1000 negatius)\n",
    "print(mr.words('pos/cv000_29590.txt'))\n",
    "print(mr.categories())\n",
    "print(mr.fileids('pos')) #lista todos los ficheros positivos\n",
    "print((list(mr.words('pos/cv000_29590.txt')), 'pos') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particion Train y Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_pos = [mr.words(f) for f in mr.fileids('pos')]\n",
    "doc_neg = [mr.words(f) for f in mr.fileids('neg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(doc_pos)\n",
    "random.shuffle(doc_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = doc_pos[:int((len(doc_pos)*0.7))] #70% para el train\n",
    "test_pos = doc_pos[int((len(doc_pos)*0.7)):] #30% final para el test\n",
    "\n",
    "train_neg = doc_neg[:int((len(doc_neg)*0.7))]\n",
    "test_neg = doc_neg[int((len(doc_neg)*0.7)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_pos + train_neg\n",
    "test = test_pos + test_neg\n",
    "#hay que guardar estos ficheros porque cada vez que ejecutemos el codigo el random va a cambiar el contenido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_pos), len(train_neg))\n",
    "print(len(test_pos), len(test_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = [1]*700 + [0]*700\n",
    "print(len(train_labels))\n",
    "test_labels = [1]*300 + [0]*300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preproces(text):\n",
    "    text_clean = []\n",
    "    # Expresión regular para eliminar todo lo que no sea una letra o un espacio\n",
    "    regex = r\"[^a-zA-Z\\s]\"\n",
    "    \n",
    "    # Diccionario de reemplazo\n",
    "    replacements = {\n",
    "        \"t\": \"not\",\n",
    "        's': 'is', #aunque tambien puede ser posesivo\n",
    "        \"didn\": \"did\",\n",
    "        \"haven\": \"have\",\n",
    "        \"hasn\": \"has\",\n",
    "        \"hadn\": \"had\",\n",
    "        \"shouldn\": \"should\",\n",
    "        \"wouldn\": \"would\",\n",
    "        \"couldn\": \"could\",\n",
    "        \"mustn\": \"must\",\n",
    "        'doesn': 'does',\n",
    "        'isn' : 'is',\n",
    "        'aren' : 'are',\n",
    "        'll': 'will',\n",
    "        've' : 'have', \n",
    "        'd' : 'would',\n",
    "        'm': 'am'\n",
    "    }\n",
    "    \n",
    "    for w in text:\n",
    "        # Eliminar caracteres no deseados y números\n",
    "        limpio = re.sub(regex, \"\", w)\n",
    "        # Convertir a minúsculas\n",
    "        limpio = limpio.lower()\n",
    "        # Eliminar espacios en blanco al principio y al final\n",
    "        limpio = limpio.strip()\n",
    "        # Eliminar si la palabra es \"s\"\n",
    "        #if limpio == \"s\":\n",
    "           # continue\n",
    "        # Reemplazar si está en el diccionario\n",
    "        if limpio in replacements:\n",
    "            limpio = replacements[limpio]\n",
    "        # Filtrar tokens vacíos\n",
    "        if limpio != \"\":\n",
    "            text_clean.append(limpio)\n",
    "    \n",
    "    return text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train = [preproces(words) for words in train]\n",
    "print(clean_train[0])\n",
    "clean_test = [preproces(words)for words in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lematizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Poner tags y no incluir stop Words ya que no aportaran informacion importante para el Sentiment Analisys\n",
    "train_tagged = []\n",
    "tags = set()\n",
    "stop_tags = {'DT', 'IN', 'CC', 'PRP', 'PRP$', 'WP', 'WP$', 'RP', 'TO', 'CD', 'EX', 'WDT'}\n",
    "for l in clean_train:\n",
    "    aux = []\n",
    "    con_tag = nltk.pos_tag(l)\n",
    "    for w, tag in con_tag:\n",
    "        if tag not in stop_tags:\n",
    "            aux.append((w, tag))\n",
    "            tags.add(tag)\n",
    "    train_tagged.append(aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tags) #todos los tags unicos que aparecen en el train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ejemplo del resultado\n",
    "print(train_tagged[0])\n",
    "print(train[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ver otros ejemplos de palabras con un cierto tag\n",
    "palabras = []\n",
    "for l in train_tagged:\n",
    "    for palabra, etiqueta in l:\n",
    "            if etiqueta == '$':\n",
    "                palabras.append(palabra)\n",
    "print(palabras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El tag '$' es un error del tagger pero no upondrá ningún problema. Además, al observar el tag \"FW\" (palabras que se etiquetan como extranjeras) vemos que podríamos eliminar errores, palabras en otros idiomas, nombres propios, pero también palabras normales. Como posteriormnte se va a hacer filtrado de frequencias esperamos que se eliminarán las palabras no deseadas y quedarán las que sí que tienen sentido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se lematizan solo estas categorias, reconocidas por SentiWordNet:\n",
    "\n",
    "Sustantivos ('n')\t'NN', 'NNS', 'NNP', 'NNPS'\n",
    "Verbos ('v')\t'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'\n",
    "Adjetivos ('a')\t'JJ', 'JJR', 'JJS'\n",
    "Adverbios ('r')\t'RB', 'RBR', 'RBS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "def lemmatize(p):\n",
    "  d = {'NN': 'n', 'NNS': 'n', 'NNP' : 'n', 'NNPS':'n', #sustantivos\n",
    "       'JJ': 'a', 'JJR': 'a', 'JJS': 'a',  #adjetivos \n",
    "       'VB': 'v', 'VBD': 'v', 'VBG': 'v', 'VBN': 'v', 'VBP': 'v', 'VBZ': 'v', 'MD':'v', #verbos\n",
    "       'RB': 'r', 'RBR': 'r', 'RBS': 'r'}  #adverbios\n",
    "  if p[1] in d:\n",
    "    return wnl.lemmatize(p[0], pos=d[p[1]])\n",
    "  return p[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lemat = []\n",
    "for l in train_tagged:\n",
    "    aux = []\n",
    "    for i in l:\n",
    "        aux.append(lemmatize(i))\n",
    "    train_lemat.append(aux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectroización BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "train_aplanado = [' '.join(sublista) for sublista in train_lemat]\n",
    "X_train = vectorizer.fit_transform(train_aplanado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aplanado[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminacion de frequencias bajas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sumamos las frecuencias de cada palabra en todos los documentos\n",
    "word_frequencies = X_train.sum(axis=0).A1\n",
    "# Obtenemos el vocabulario mapeado a índices\n",
    "words = vectorizer.get_feature_names_out()\n",
    "# Convertimos en un diccionario {palabra: frecuencia}\n",
    "freq_dict = dict(zip(words, word_frequencies))\n",
    "# Ordenamos por frecuencia ascendente\n",
    "sorted_freq = sorted(freq_dict.items(), key=lambda x: x[1])\n",
    "print(sorted_freq[:10])  # Muestra las 10 palabras menos frecuentes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Ordenamos las frecuencias en orden descendente\n",
    "sorted_freq_values = np.array(sorted(word_frequencies, reverse=True))\n",
    "\n",
    "# Creamos la gráfica\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(sorted_freq_values, marker=\"o\", linestyle=\"-\")\n",
    "\n",
    "# Agregamos etiquetas\n",
    "plt.xlabel(\"Palabras ordenadas por frecuencia\")\n",
    "plt.ylabel(\"Frecuencia de aparición\")\n",
    "plt.title(\"Distribución de frecuencias de palabras\")\n",
    "plt.yscale(\"log\") \n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Histograma detallado con límite en eje X\n",
    "max_freq = int(max(word_frequencies))\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.hist(word_frequencies, bins=np.arange(1, max_freq + 1), edgecolor='black', log=True)\n",
    "\n",
    "plt.xlabel(\"Frecuencia exacta\")\n",
    "plt.ylabel(\"Cantidad de palabras\")\n",
    "plt.title(\"Histograma de frecuencias de palabras (zoom 0–100)\")\n",
    "plt.grid(axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_freq = int(max(word_frequencies))\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.hist(word_frequencies, bins=np.arange(1, max_freq + 1), edgecolor='black', log=True)\n",
    "\n",
    "plt.xlabel(\"Frecuencia exacta\")\n",
    "plt.ylabel(\"Cantidad de palabras\")\n",
    "plt.title(\"Histograma de frecuencias de palabras (zoom 0–100)\")\n",
    "plt.grid(axis='y')\n",
    "\n",
    "# 🔍 Zoom en el eje X\n",
    "plt.xlim(0, 50)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 3  # Umbral mínimo (si se pone mas agrande se eliminaran mas de la mitad de las observaciones)\n",
    "vectorizer = CountVectorizer(min_df=min_freq)\n",
    "X_filtered = vectorizer.fit_transform(train_aplanado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies = X_filtered.sum(axis=0).A1\n",
    "sorted_freq_values = np.array(sorted(word_frequencies, reverse=True))\n",
    "\n",
    "# Creamos la gráfica\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(sorted_freq_values, marker=\"o\", linestyle=\"-\")\n",
    "\n",
    "# Agregamos etiquetas\n",
    "plt.xlabel(\"Palabras ordenadas por frecuencia\")\n",
    "plt.ylabel(\"Frecuencia de aparición filtarada\")\n",
    "plt.title(\"Distribución de frecuencias de palabras\")\n",
    "plt.yscale(\"log\") \n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_filtered.shape #train final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos supervisados. Parte 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que ya tengamos los datos preprocesados de manera que faciliten a los modelos a classificar los documentos, empezaremos a construir los modelos supervisados. \n",
    "\n",
    "Para esta práctica hemos querido comparar tres modelos supervisados significativamente diferentes entre ellos: KNN, Random Forest y SVM (Support Vector Machine). Para cada uno de los modelos se contemplará el mismo procedimiento de entrenamiento. \n",
    "\n",
    "Primero de todo, se usará la función **GridSearchCV** para encontrar los mejores parámetros que se ajusten a los datos del train, posteriormente se aplicará la función cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "\n",
    "# Tus datos\n",
    "X = train_aplanado\n",
    "y = train_labels\n",
    "\n",
    "# Métricas que quieres evaluar\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'f1_macro': 'f1_macro',\n",
    "    'precision_macro': 'precision_macro',\n",
    "    'recall_macro': 'recall_macro'\n",
    "}\n",
    "\n",
    "# Resultados globales\n",
    "resultados = {}\n",
    "\n",
    "# ----------- MODELO 1: KNN -----------\n",
    "pipe_knn = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(min_df=3)),\n",
    "    ('clf', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "param_knn = {\n",
    "    'clf__n_neighbors': [3, 5, 7]\n",
    "}\n",
    "\n",
    "grid_knn = GridSearchCV(pipe_knn, param_grid=param_knn, cv=5, scoring='accuracy')\n",
    "grid_knn.fit(X, y)\n",
    "\n",
    "mejor_knn = grid_knn.best_estimator_\n",
    "\n",
    "# ----------- MODELO 2: Random Forest -----------\n",
    "pipe_rf = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(min_df=3)),\n",
    "    ('clf', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "param_rf = {\n",
    "    'clf__n_estimators': [50, 100],\n",
    "    'clf__max_depth': [None, 10, 100]\n",
    "}\n",
    "\n",
    "grid_rf = GridSearchCV(pipe_rf, param_grid=param_rf, cv=5, scoring='accuracy')\n",
    "grid_rf.fit(X, y)\n",
    "mejor_rf = grid_rf.best_estimator_\n",
    "\n",
    "# ----------- MODELO 3: SVM -----------\n",
    "pipe_svm = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(min_df=3)),\n",
    "    ('clf', SVC())\n",
    "])\n",
    "\n",
    "param_svm = {\n",
    "    'clf__C': [0.1, 1, 10],\n",
    "    'clf__kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "grid_svm = GridSearchCV(pipe_svm, param_grid=param_svm, cv=5, scoring='accuracy')\n",
    "grid_svm.fit(X, y)\n",
    "mejor_svm = grid_svm.best_estimator_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Diccionario de modelos a comparar\n",
    "modelos = {\n",
    "    \"KNN\": mejor_knn,\n",
    "    \"RandomForest\": mejor_rf,\n",
    "    \"SVM\": mejor_svm\n",
    "}\n",
    "\n",
    "# Guardamos resultados\n",
    "resultados = {}\n",
    "\n",
    "# mejor_knn, mejor_rf, mejor_svm son Pipelines que contienen el mejor modelo y el vectorizador. \n",
    "for nombre, modelo in modelos.items():\n",
    "    cv_result = cross_validate(modelo, X, y, cv=5, scoring=scoring)\n",
    "\n",
    "    resumen = {métrica: round(cv_result[f'test_{métrica}'].mean(), 4) for métrica in scoring}\n",
    "    resultados[nombre] = resumen\n",
    "\n",
    "\n",
    "# Mostrar como DataFrame ordenado\n",
    "df_resultados = pd.DataFrame(resultados).T\n",
    "print(\"🔍 Los mejores resultados de cada modelo:\")\n",
    "print(df_resultados)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos no supervisados. Parte 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from textserver import TextServer\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtar train_lemat\n",
    "\n",
    "word_freq = Counter()\n",
    "for doc in train_lemat:\n",
    "    word_freq.update(doc)\n",
    "\n",
    "min_freq = 5\n",
    "filtered_words = {word for word, freq in word_freq.items() if freq < min_freq}\n",
    "filtered_train_lemat = [\n",
    "    [word for word in doc if word not in filtered_words]\n",
    "    for doc in train_lemat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "ts = TextServer('taisiia.prymak', '', 'senses') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ts.senses('lively')[0][0]\n",
    "print(result)\n",
    "print(lesk(train_lemat[8], 'lively'))\n",
    "#hay palabras que Freeling es incapaz de encontrar su synset, pero lesk si"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si Freeling no encuentra el synset intentaremos encontrarlo con Lesk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synsets(w, contexto):\n",
    "    try:\n",
    "        result = ts.senses(w)[0][0]\n",
    "        categ = result[4][-1]\n",
    "        id = int(result[4][0:8])\n",
    "        synset1 = wn.synset_from_pos_and_offset(categ, id)\n",
    "        return synset1\n",
    "    except:\n",
    "        return lesk(contexto, w) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filtered_train_lemat[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prueba = [filtered_train_lemat[0]]\n",
    "print(prueba)\n",
    "etiquetas_predichas = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metodo simple: Clasificaci´on de polaridad: score(s) = poss − neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asignación de valores neutros\n",
    "\n",
    "Enfoque: Asignar un valor neutral (0) a todos los synsets no encontrados.\n",
    "\n",
    "Justificación académica: Este enfoque asume que las palabras ausentes no contribuyen al sentimiento del texto. Puedes argumentar que SentiWordNet se enfoca en palabras con carga emocional clara, y por tanto, las palabras ausentes probablemente sean neutras en términos de sentimiento.\n",
    "\n",
    "Cita potencial: \"La ausencia de ciertos términos en léxicos de sentimiento como SentiWordNet puede interpretarse como evidencia de su naturaleza semánticamente neutra en contextos evaluativos\" (Baccianella et al., 2010)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#else: si el sinset NO esta en sentiWordNet se va a suponer que su puntuacion es 0 JUSTIFICAR \n",
    "#si es neutral clasificar como positivo?????????????????????????????????????????????????????????????????????\n",
    "\n",
    "def predecir_sentimiento(lista_docs):\n",
    "\n",
    "    for doc in lista_docs:\n",
    "        pos_s = 0\n",
    "        neg_s = 0\n",
    "\n",
    "        for w in doc:\n",
    "            s1 = get_synsets(w, doc)\n",
    "            if s1 is not None: #si ni freeling ni lesk ha encontrado el syntet la funcion get_synsets(w, doc) devuelve None\n",
    "                try:\n",
    "                    senti_synset = swn.senti_synset(s1.name())\n",
    "                    pos_s += senti_synset.pos_score()\n",
    "                    neg_s += senti_synset.neg_score()\n",
    "                except:\n",
    "                    pass  # si no está en SentiWordNet\n",
    "        etiquetas_predichas.append(1 if pos_s - neg_s >= 0 else 0)\n",
    "\n",
    "    return etiquetas_predichas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for l in tqdm(filtered_train_lemat):\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define etiquetas útiles para sentimiento\n",
    "POS_RELEVANTES = {'JJ', 'JJR', 'JJS',  # Adjetivos\n",
    "                  'NN', 'NNS', 'NNP', 'NNPS',  # Sustantivos\n",
    "                  'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ',  # Verbos\n",
    "                  'RB', 'RBR', 'RBS'}  # Adverbios\n",
    "\n",
    "SOLO_ADJETIVOS = {'JJ', 'JJR', 'JJS'}\n",
    "\n",
    "def filtrar_tagged_por_pos(tagged_docs, etiquetas_validas):\n",
    "    return [[w for w, tag in doc if tag in etiquetas_validas] for doc in tagged_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "568794\n"
     ]
    }
   ],
   "source": [
    "docs_relevantes = filtrar_tagged_por_pos(train_tagged, POS_RELEVANTES)\n",
    "print(sum(len(doc) for doc in docs_relevantes))\n",
    "preds_relevantes = predecir_sentimiento(docs_relevantes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_adjetivos = filtrar_tagged_por_pos(train_tagged, SOLO_ADJETIVOS)\n",
    "preds_adjetivos = predecir_sentimiento(docs_adjetivos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"🔍 Evaluación con POS relevantes:\")\n",
    "print(classification_report(train_labels, preds_relevantes))\n",
    "\n",
    "print(\"\\n🔍 Evaluación con solo adjetivos:\")\n",
    "print(classification_report(train_labels, preds_adjetivos))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Evaluando LogisticRegression...\n",
      "🧪 Evaluando RandomForest...\n",
      "\n",
      "📊 Resultados promedio con cross-validation:\n",
      "                    accuracy  f1_macro  precision_macro  recall_macro\n",
      "LogisticRegression    0.6543    0.6540           0.6547        0.6543\n",
      "RandomForest          0.6371    0.6361           0.6390        0.6371\n",
      "\n",
      "🎯 Pesos finales entrenando con todo el dataset:\n",
      "\n",
      "Coeficientes (LogisticRegression):\n",
      "adj: 0.1895\n",
      "verb: 0.1000\n",
      "adv: 0.1356\n",
      "noun: 0.0084\n",
      "\n",
      "Importancia por feature (RandomForest):\n",
      "adj: 0.3100\n",
      "verb: 0.2252\n",
      "adv: 0.2365\n",
      "noun: 0.2283\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.wsd import lesk\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# POS relevantes\n",
    "POS_TAGS = {\n",
    "    'adj': {'JJ', 'JJR', 'JJS'},\n",
    "    'verb': {'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'},\n",
    "    'adv': {'RB', 'RBR', 'RBS'},\n",
    "    'noun': {'NN', 'NNS', 'NNP', 'NNPS'}\n",
    "}\n",
    "\n",
    "def extract_pos_sentiment_features(tagged_docs):\n",
    "    X = []\n",
    "    for doc in tagged_docs:\n",
    "        scores = {'adj': 0, 'verb': 0, 'adv': 0, 'noun': 0}\n",
    "        tokens = [w for w, _ in doc]\n",
    "\n",
    "        for w, tag in doc:\n",
    "            pos_category = None\n",
    "            for key, tag_set in POS_TAGS.items():\n",
    "                if tag in tag_set:\n",
    "                    pos_category = key\n",
    "                    break\n",
    "            if pos_category:\n",
    "                syn = lesk(tokens, w)\n",
    "                if syn:\n",
    "                    try:\n",
    "                        senti = swn.senti_synset(syn.name())\n",
    "                        scores[pos_category] += senti.pos_score() - senti.neg_score()\n",
    "                    except:\n",
    "                        pass\n",
    "        X.append([scores['adj'], scores['verb'], scores['adv'], scores['noun']])\n",
    "    return np.array(X)\n",
    "\n",
    "# Extraemos features\n",
    "X_features = extract_pos_sentiment_features(train_tagged)\n",
    "y = train_labels\n",
    "\n",
    "# Métricas a evaluar\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'f1_macro': 'f1_macro',\n",
    "    'precision_macro': 'precision_macro',\n",
    "    'recall_macro': 'recall_macro'\n",
    "}\n",
    "\n",
    "# Modelos\n",
    "modelos = {\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=1000),\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Resultados\n",
    "resultados = {}\n",
    "\n",
    "for nombre, modelo in modelos.items():\n",
    "    print(f\"🧪 Evaluando {nombre}...\")\n",
    "    cv_result = cross_validate(modelo, X_features, y, cv=5, scoring=scoring)\n",
    "    resumen = {métrica: round(np.mean(cv_result[f'test_{métrica}']), 4) for métrica in scoring}\n",
    "    resultados[nombre] = resumen\n",
    "\n",
    "# Mostrar tabla de resultados\n",
    "df_resultados = pd.DataFrame(resultados).T\n",
    "print(\"\\n📊 Resultados promedio con cross-validation:\")\n",
    "print(df_resultados)\n",
    "\n",
    "# ENTRENAR FINAL para mostrar coeficientes / importancias\n",
    "print(\"\\n🎯 Pesos finales entrenando con todo el dataset:\")\n",
    "\n",
    "lr_model = modelos[\"LogisticRegression\"].fit(X_features, y)\n",
    "rf_model = modelos[\"RandomForest\"].fit(X_features, y)\n",
    "\n",
    "# Pesos LR\n",
    "print(\"\\nCoeficientes (LogisticRegression):\")\n",
    "for i, pos in enumerate(['adj', 'verb', 'adv', 'noun']):\n",
    "    print(f\"{pos}: {lr_model.coef_[0][i]:.4f}\")\n",
    "\n",
    "# Importancias RF\n",
    "print(\"\\nImportancia por feature (RandomForest):\")\n",
    "for i, pos in enumerate(['adj', 'verb', 'adv', 'noun']):\n",
    "    print(f\"{pos}: {rf_model.feature_importances_[i]:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
